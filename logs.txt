* 
* ==> Audit <==
* |--------------|--------------------------------|------------|-------|---------|---------------------|---------------------|
|   Command    |              Args              |  Profile   | User  | Version |     Start Time      |      End Time       |
|--------------|--------------------------------|------------|-------|---------|---------------------|---------------------|
| start        | --nodes 4 -p django-k8s        | django-k8s | burak | v1.27.0 | 09 Oct 22 18:13 +03 | 09 Oct 22 18:14 +03 |
| update-check |                                | minikube   | burak | v1.27.0 | 09 Oct 22 18:19 +03 | 09 Oct 22 18:19 +03 |
| tunnel       |                                | minikube   | burak | v1.27.0 | 09 Oct 22 19:14 +03 |                     |
| profile      | list                           | minikube   | burak | v1.27.0 | 09 Oct 22 19:14 +03 | 09 Oct 22 19:14 +03 |
| service      | --url nginx-service            | minikube   | burak | v1.27.0 | 09 Oct 22 20:13 +03 |                     |
| start        | nodes=4                        | minikube   | burak | v1.27.0 | 10 Oct 22 05:12 +03 | 10 Oct 22 05:13 +03 |
| delete       | --all                          | minikube   | burak | v1.27.0 | 10 Oct 22 05:14 +03 | 10 Oct 22 05:14 +03 |
| start        | --nodes=4                      | minikube   | burak | v1.27.0 | 10 Oct 22 05:14 +03 | 10 Oct 22 05:15 +03 |
| tunnel       |                                | minikube   | burak | v1.27.0 | 10 Oct 22 05:17 +03 |                     |
| update-check |                                | minikube   | burak | v1.27.1 | 10 Oct 22 05:22 +03 | 10 Oct 22 05:22 +03 |
| update-check |                                | minikube   | burak | v1.27.1 | 10 Oct 22 22:41 +03 | 10 Oct 22 22:41 +03 |
| update-check |                                | minikube   | burak | v1.27.1 | 11 Oct 22 00:20 +03 | 11 Oct 22 00:20 +03 |
| update-check |                                | minikube   | burak | v1.27.1 | 11 Oct 22 01:01 +03 | 11 Oct 22 01:01 +03 |
| update-check |                                | minikube   | burak | v1.27.1 | 11 Oct 22 01:24 +03 | 11 Oct 22 01:24 +03 |
| delete       | --all                          | minikube   | burak | v1.27.0 | 11 Oct 22 01:38 +03 | 11 Oct 22 01:39 +03 |
| update-check |                                | minikube   | burak | v1.27.1 | 11 Oct 22 03:34 +03 | 11 Oct 22 03:34 +03 |
| start        | --nodes=4                      | minikube   | burak | v1.27.0 | 11 Oct 22 11:34 +03 | 11 Oct 22 11:36 +03 |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:03 +03 |                     |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:44 +03 |                     |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:46 +03 |                     |
| addons       | configure registry-creds       | minikube   | burak | v1.27.0 | 11 Oct 22 12:46 +03 |                     |
| addons       | configure registry-creds       | minikube   | burak | v1.27.0 | 11 Oct 22 12:46 +03 |                     |
| addons       | enable registry                | minikube   | burak | v1.27.0 | 11 Oct 22 12:47 +03 | 11 Oct 22 12:48 +03 |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:48 +03 |                     |
| dashboard    |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:52 +03 |                     |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:55 +03 |                     |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:57 +03 |                     |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:57 +03 |                     |
| docker-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:58 +03 |                     |
| cache        | add alpine:latest              | minikube   | burak | v1.27.0 | 11 Oct 22 12:58 +03 | 11 Oct 22 12:59 +03 |
| podman-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:59 +03 |                     |
| addons       | enable registry                | minikube   | burak | v1.27.0 | 11 Oct 22 12:59 +03 | 11 Oct 22 12:59 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 12:59 +03 | 11 Oct 22 12:59 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:00 +03 | 11 Oct 22 13:00 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:01 +03 | 11 Oct 22 13:01 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:01 +03 | 11 Oct 22 13:01 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:02 +03 | 11 Oct 22 13:02 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:02 +03 | 11 Oct 22 13:02 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:02 +03 | 11 Oct 22 13:02 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:02 +03 | 11 Oct 22 13:02 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:03 +03 | 11 Oct 22 13:03 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:04 +03 | 11 Oct 22 13:04 +03 |
| ip           |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:04 +03 | 11 Oct 22 13:04 +03 |
| podman-env   |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:05 +03 |                     |
| image        | ls                             | minikube   | burak | v1.27.0 | 11 Oct 22 13:06 +03 | 11 Oct 22 13:06 +03 |
| image        | build .                        | minikube   | burak | v1.27.0 | 11 Oct 22 13:07 +03 | 11 Oct 22 13:08 +03 |
| image        | ls                             | minikube   | burak | v1.27.0 | 11 Oct 22 13:08 +03 | 11 Oct 22 13:08 +03 |
| image        | build -t django-k8s-web:latest | minikube   | burak | v1.27.0 | 11 Oct 22 13:08 +03 | 11 Oct 22 13:08 +03 |
|              | .                              |            |       |         |                     |                     |
| image        | ls                             | minikube   | burak | v1.27.0 | 11 Oct 22 13:08 +03 | 11 Oct 22 13:09 +03 |
| image        | load .                         | minikube   | burak | v1.27.0 | 11 Oct 22 13:09 +03 | 11 Oct 22 13:09 +03 |
| image        | ls                             | minikube   | burak | v1.27.0 | 11 Oct 22 13:10 +03 | 11 Oct 22 13:10 +03 |
| tunnel       |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:22 +03 | 11 Oct 22 13:38 +03 |
| tunnel       |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:38 +03 |                     |
| tunnel       |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:46 +03 |                     |
| tunnel       | -c                             | minikube   | burak | v1.27.0 | 11 Oct 22 13:47 +03 |                     |
| tunnel       | --cleanup                      | minikube   | burak | v1.27.0 | 11 Oct 22 13:47 +03 |                     |
| delete       | --all                          | minikube   | burak | v1.27.0 | 11 Oct 22 13:48 +03 | 11 Oct 22 13:48 +03 |
| tunnel       |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:49 +03 |                     |
| start        | --nodes=4                      | minikube   | burak | v1.27.0 | 11 Oct 22 13:49 +03 | 11 Oct 22 13:50 +03 |
| tunnel       |                                | minikube   | burak | v1.27.0 | 11 Oct 22 13:57 +03 |                     |
|--------------|--------------------------------|------------|-------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/10/11 13:49:09
Running on machine: burak
Binary: Built with gc go1.19.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1011 13:49:09.371151  311659 out.go:296] Setting OutFile to fd 1 ...
I1011 13:49:09.371226  311659 out.go:348] isatty.IsTerminal(1) = true
I1011 13:49:09.371229  311659 out.go:309] Setting ErrFile to fd 2...
I1011 13:49:09.371232  311659 out.go:348] isatty.IsTerminal(2) = true
I1011 13:49:09.371477  311659 root.go:333] Updating PATH: /home/burak/.minikube/bin
I1011 13:49:09.372094  311659 out.go:303] Setting JSON to false
I1011 13:49:09.391066  311659 start.go:115] hostinfo: {"hostname":"burak","uptime":37054,"bootTime":1665448296,"procs":704,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.15.0-50-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"451083e1-d8a9-49a2-99d8-758a9bfc56ec"}
I1011 13:49:09.391125  311659 start.go:125] virtualization: kvm host
I1011 13:49:09.394417  311659 out.go:177] üòÑ  minikube v1.27.0 on Ubuntu 20.04
I1011 13:49:09.400183  311659 notify.go:214] Checking for updates...
W1011 13:49:09.400257  311659 out.go:239] ‚ùó  Kubernetes 1.25.0 has a known issue with resolv.conf. minikube is using a workaround that should work for most use cases.
W1011 13:49:09.400330  311659 out.go:239] ‚ùó  For more information, see: https://github.com/kubernetes/kubernetes/issues/112135
I1011 13:49:09.401518  311659 driver.go:365] Setting default libvirt URI to qemu:///system
I1011 13:49:09.401570  311659 global.go:111] Querying for installed drivers using PATH=/home/burak/.minikube/bin:/home/burak/.local/bin:/home/burak/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
I1011 13:49:09.401642  311659 global.go:119] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1011 13:49:09.401725  311659 global.go:119] vmware default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker-machine-driver-vmware": executable file not found in $PATH Reason: Fix:Install docker-machine-driver-vmware Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1011 13:49:09.433613  311659 docker.go:137] docker version: linux-20.10.18
I1011 13:49:09.433674  311659 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1011 13:49:09.502233  311659 info.go:265] docker info: {ID:R4JC:EJUZ:DPJV:EH2R:UNJ5:HAPB:LZTF:3E4K:H4OA:WBNS:3LRG:AVNT Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:32 OomKillDisable:true NGoroutines:40 SystemTime:2022-10-11 13:49:09.451472692 +0300 +03 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-50-generic OperatingSystem:Ubuntu 20.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8065196032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:burak Labels:[] ExperimentalBuild:false ServerVersion:20.10.18 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.10.2] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1011 13:49:09.502295  311659 docker.go:254] overlay module found
I1011 13:49:09.502300  311659 global.go:119] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1011 13:49:09.502349  311659 global.go:119] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1011 13:49:09.507696  311659 global.go:119] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1011 13:49:09.507733  311659 global.go:119] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1011 13:49:09.507773  311659 global.go:119] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1011 13:49:09.507779  311659 global.go:119] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1011 13:49:09.507792  311659 driver.go:300] not recommending "none" due to default: false
I1011 13:49:09.507795  311659 driver.go:300] not recommending "ssh" due to default: false
I1011 13:49:09.507805  311659 driver.go:335] Picked: docker
I1011 13:49:09.507809  311659 driver.go:336] Alternatives: [none ssh]
I1011 13:49:09.507811  311659 driver.go:337] Rejects: [virtualbox vmware kvm2 podman qemu2]
I1011 13:49:09.510842  311659 out.go:177] ‚ú®  Automatically selected the docker driver. Other choices: none, ssh
I1011 13:49:09.513591  311659 start.go:284] selected driver: docker
I1011 13:49:09.513599  311659 start.go:808] validating driver "docker" against <nil>
I1011 13:49:09.513615  311659 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1011 13:49:09.513685  311659 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1011 13:49:09.587008  311659 info.go:265] docker info: {ID:R4JC:EJUZ:DPJV:EH2R:UNJ5:HAPB:LZTF:3E4K:H4OA:WBNS:3LRG:AVNT Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:32 OomKillDisable:true NGoroutines:40 SystemTime:2022-10-11 13:49:09.52967103 +0300 +03 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-50-generic OperatingSystem:Ubuntu 20.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8065196032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:burak Labels:[] ExperimentalBuild:false ServerVersion:20.10.18 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.10.2] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1011 13:49:09.587076  311659 start_flags.go:296] no existing cluster config was found, will generate one from the flags 
I1011 13:49:09.587407  311659 start_flags.go:377] Using suggested 2200MB memory alloc based on sys=7691MB, container=7691MB
I1011 13:49:09.587491  311659 start_flags.go:835] Wait components to verify : map[apiserver:true system_pods:true]
I1011 13:49:09.590335  311659 out.go:177] üìå  Using Docker driver with root privileges
I1011 13:49:09.593136  311659 cni.go:95] Creating CNI manager for ""
I1011 13:49:09.593146  311659 cni.go:156] 0 nodes found, recommending kindnet
I1011 13:49:09.593155  311659 start_flags.go:305] Found "CNI" CNI - setting NetworkPlugin=cni
I1011 13:49:09.593168  311659 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1011 13:49:09.595624  311659 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1011 13:49:09.598459  311659 cache.go:120] Beginning downloading kic base image for docker with docker
I1011 13:49:09.601345  311659 out.go:177] üöú  Pulling base image ...
I1011 13:49:09.604388  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:49:09.604436  311659 preload.go:148] Found local preload: /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4
I1011 13:49:09.604441  311659 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I1011 13:49:09.604443  311659 cache.go:57] Caching tarball of preloaded images
I1011 13:49:09.604576  311659 preload.go:174] Found /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1011 13:49:09.604582  311659 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.0 on docker
I1011 13:49:09.604819  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:49:09.604831  311659 lock.go:35] WriteFile acquiring /home/burak/.minikube/profiles/minikube/config.json: {Name:mkaee370099a8c13f788cb73817792a4bdc72173 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:09.640830  311659 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I1011 13:49:09.640840  311659 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I1011 13:49:09.640848  311659 cache.go:208] Successfully downloaded all kic artifacts
I1011 13:49:09.640871  311659 start.go:364] acquiring machines lock for minikube: {Name:mkda6085529bbfaa17a1d1a96095c7d735afeea8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:49:09.640940  311659 start.go:368] acquired machines lock for "minikube" in 58.711¬µs
I1011 13:49:09.640950  311659 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name: IP: Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1011 13:49:09.641002  311659 start.go:125] createHost starting for "" (driver="docker")
I1011 13:49:09.643127  311659 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I1011 13:49:09.643489  311659 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1011 13:49:09.643508  311659 client.go:168] LocalClient.Create starting
I1011 13:49:09.643681  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/ca.pem
I1011 13:49:09.643817  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:49:09.643829  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:49:09.643887  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/cert.pem
I1011 13:49:09.643986  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:49:09.643994  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:49:09.644500  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1011 13:49:09.668588  311659 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1011 13:49:09.668628  311659 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I1011 13:49:09.668638  311659 cli_runner.go:164] Run: docker network inspect minikube
W1011 13:49:09.687546  311659 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1011 13:49:09.687559  311659 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1011 13:49:09.687568  311659 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I1011 13:49:09.687609  311659 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:49:09.706135  311659 network.go:290] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc0004f5170] misses:0}
I1011 13:49:09.706297  311659 network.go:236] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I1011 13:49:09.706319  311659 network_create.go:115] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1011 13:49:09.706372  311659 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1011 13:49:09.783213  311659 network_create.go:99] docker network minikube 192.168.49.0/24 created
I1011 13:49:09.783232  311659 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I1011 13:49:09.783293  311659 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1011 13:49:09.815939  311659 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1011 13:49:09.844816  311659 oci.go:103] Successfully created a docker volume minikube
I1011 13:49:09.844863  311659 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I1011 13:49:10.906717  311659 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib: (1.061789228s)
I1011 13:49:10.906751  311659 oci.go:107] Successfully prepared a docker volume minikube
I1011 13:49:10.906794  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:49:10.906825  311659 kic.go:179] Starting extracting preloaded images to volume ...
I1011 13:49:10.906936  311659 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
I1011 13:49:13.744110  311659 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (2.837075459s)
I1011 13:49:13.744157  311659 kic.go:188] duration metric: took 2.837326 seconds to extract preloaded images to volume
W1011 13:49:13.745051  311659 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1011 13:49:13.745257  311659 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1011 13:49:13.885196  311659 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I1011 13:49:14.369297  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1011 13:49:14.400271  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:14.428556  311659 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1011 13:49:14.488082  311659 oci.go:144] the created container "minikube" has a running status.
I1011 13:49:14.488096  311659 kic.go:210] Creating ssh key for kic: /home/burak/.minikube/machines/minikube/id_rsa...
I1011 13:49:14.743298  311659 kic_runner.go:191] docker (temp): /home/burak/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1011 13:49:14.869890  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:14.907711  311659 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1011 13:49:14.907722  311659 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1011 13:49:15.016859  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:15.040220  311659 machine.go:88] provisioning docker machine ...
I1011 13:49:15.040239  311659 ubuntu.go:169] provisioning hostname "minikube"
I1011 13:49:15.040281  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:15.066551  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:15.066688  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49177 <nil> <nil>}
I1011 13:49:15.066695  311659 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1011 13:49:15.202816  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1011 13:49:15.202868  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:15.229715  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:15.229847  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49177 <nil> <nil>}
I1011 13:49:15.229862  311659 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1011 13:49:15.364027  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1011 13:49:15.364066  311659 ubuntu.go:175] set auth options {CertDir:/home/burak/.minikube CaCertPath:/home/burak/.minikube/certs/ca.pem CaPrivateKeyPath:/home/burak/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/burak/.minikube/machines/server.pem ServerKeyPath:/home/burak/.minikube/machines/server-key.pem ClientKeyPath:/home/burak/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/burak/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/burak/.minikube}
I1011 13:49:15.364124  311659 ubuntu.go:177] setting up certificates
I1011 13:49:15.364146  311659 provision.go:83] configureAuth start
I1011 13:49:15.364271  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1011 13:49:15.446075  311659 provision.go:138] copyHostCerts
I1011 13:49:15.446188  311659 exec_runner.go:144] found /home/burak/.minikube/ca.pem, removing ...
I1011 13:49:15.446201  311659 exec_runner.go:207] rm: /home/burak/.minikube/ca.pem
I1011 13:49:15.446290  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/ca.pem --> /home/burak/.minikube/ca.pem (1074 bytes)
I1011 13:49:15.446518  311659 exec_runner.go:144] found /home/burak/.minikube/cert.pem, removing ...
I1011 13:49:15.446533  311659 exec_runner.go:207] rm: /home/burak/.minikube/cert.pem
I1011 13:49:15.446616  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/cert.pem --> /home/burak/.minikube/cert.pem (1119 bytes)
I1011 13:49:15.446857  311659 exec_runner.go:144] found /home/burak/.minikube/key.pem, removing ...
I1011 13:49:15.446871  311659 exec_runner.go:207] rm: /home/burak/.minikube/key.pem
I1011 13:49:15.446959  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/key.pem --> /home/burak/.minikube/key.pem (1679 bytes)
I1011 13:49:15.447644  311659 provision.go:112] generating server cert: /home/burak/.minikube/machines/server.pem ca-key=/home/burak/.minikube/certs/ca.pem private-key=/home/burak/.minikube/certs/ca-key.pem org=burak.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1011 13:49:15.680866  311659 provision.go:172] copyRemoteCerts
I1011 13:49:15.680902  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1011 13:49:15.680924  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:15.701723  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:15.812129  311659 ssh_runner.go:362] scp /home/burak/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1011 13:49:15.854488  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I1011 13:49:15.871202  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1011 13:49:15.884361  311659 provision.go:86] duration metric: configureAuth took 520.205459ms
I1011 13:49:15.884374  311659 ubuntu.go:193] setting minikube options for container-runtime
I1011 13:49:15.884477  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:49:15.884504  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:15.905135  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:15.905242  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49177 <nil> <nil>}
I1011 13:49:15.905249  311659 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1011 13:49:16.070103  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1011 13:49:16.070129  311659 ubuntu.go:71] root file system type: overlay
I1011 13:49:16.070609  311659 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1011 13:49:16.070754  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:16.117449  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:16.117557  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49177 <nil> <nil>}
I1011 13:49:16.117615  311659 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1011 13:49:16.323781  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1011 13:49:16.323931  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:16.404029  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:16.404193  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49177 <nil> <nil>}
I1011 13:49:16.404211  311659 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1011 13:49:17.390137  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-10-11 10:49:16.314187801 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1011 13:49:17.390169  311659 machine.go:91] provisioned docker machine in 2.349936356s
I1011 13:49:17.390188  311659 client.go:171] LocalClient.Create took 7.746670118s
I1011 13:49:17.390223  311659 start.go:167] duration metric: libmachine.API.Create for "minikube" took 7.746720621s
I1011 13:49:17.390235  311659 start.go:300] post-start starting for "minikube" (driver="docker")
I1011 13:49:17.390257  311659 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1011 13:49:17.390371  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1011 13:49:17.390463  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:17.466444  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:17.577543  311659 ssh_runner.go:195] Run: cat /etc/os-release
I1011 13:49:17.584819  311659 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1011 13:49:17.584858  311659 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1011 13:49:17.584883  311659 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1011 13:49:17.584893  311659 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1011 13:49:17.584912  311659 filesync.go:126] Scanning /home/burak/.minikube/addons for local assets ...
I1011 13:49:17.585147  311659 filesync.go:126] Scanning /home/burak/.minikube/files for local assets ...
I1011 13:49:17.585305  311659 start.go:303] post-start completed in 195.052226ms
I1011 13:49:17.585995  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1011 13:49:17.666936  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:49:17.667577  311659 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1011 13:49:17.667697  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:17.718086  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:17.809318  311659 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1011 13:49:17.814650  311659 start.go:128] duration metric: createHost completed in 8.173639204s
I1011 13:49:17.814659  311659 start.go:83] releasing machines lock for "minikube", held for 8.173713786s
I1011 13:49:17.814716  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1011 13:49:17.834824  311659 ssh_runner.go:195] Run: systemctl --version
I1011 13:49:17.834851  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:17.834856  311659 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1011 13:49:17.835017  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:17.857348  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:17.875123  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:17.945146  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1011 13:49:18.244163  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (234 bytes)
I1011 13:49:18.261040  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:49:18.377368  311659 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1011 13:49:18.520934  311659 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1011 13:49:18.531844  311659 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1011 13:49:18.531879  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1011 13:49:18.540969  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1011 13:49:18.551530  311659 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1011 13:49:18.648840  311659 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1011 13:49:18.746439  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:49:18.839268  311659 ssh_runner.go:195] Run: sudo systemctl restart docker
I1011 13:49:19.044485  311659 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1011 13:49:19.127114  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:49:19.281730  311659 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1011 13:49:19.288828  311659 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1011 13:49:19.288879  311659 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1011 13:49:19.291134  311659 start.go:471] Will wait 60s for crictl version
I1011 13:49:19.291161  311659 ssh_runner.go:195] Run: sudo crictl version
I1011 13:49:19.353546  311659 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1011 13:49:19.353581  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:49:19.428175  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:49:19.458019  311659 out.go:204] üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
I1011 13:49:19.458094  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:49:19.474079  311659 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1011 13:49:19.476628  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:49:19.485907  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:49:19.485947  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:49:19.515461  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:49:19.515471  311659 docker.go:542] Images already preloaded, skipping extraction
I1011 13:49:19.515508  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:49:19.539333  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:49:19.539341  311659 cache_images.go:84] Images are preloaded, skipping loading
I1011 13:49:19.539376  311659 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1011 13:49:19.680975  311659 cni.go:95] Creating CNI manager for ""
I1011 13:49:19.680983  311659 cni.go:156] 1 nodes found, recommending kindnet
I1011 13:49:19.681154  311659 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1011 13:49:19.681165  311659 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:true}
I1011 13:49:19.681263  311659 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
resolvConf: /etc/kubelet-resolv.conf
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1011 13:49:19.681316  311659 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1011 13:49:19.681347  311659 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.0
I1011 13:49:19.689017  311659 binaries.go:44] Found k8s binaries, skipping transfer
I1011 13:49:19.689051  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1011 13:49:19.693730  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1011 13:49:19.703381  311659 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1011 13:49:19.714223  311659 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2068 bytes)
I1011 13:49:19.724386  311659 ssh_runner.go:195] Run: sudo cp /etc/resolv.conf /etc/kubelet-resolv.conf
I1011 13:49:19.733404  311659 ssh_runner.go:195] Run: sudo sed -i -e "s/^search .$//" /etc/kubelet-resolv.conf
I1011 13:49:19.740796  311659 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1011 13:49:19.743990  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:49:19.752705  311659 certs.go:54] Setting up /home/burak/.minikube/profiles/minikube for IP: 192.168.49.2
I1011 13:49:19.752939  311659 certs.go:182] skipping minikubeCA CA generation: /home/burak/.minikube/ca.key
I1011 13:49:19.753057  311659 certs.go:182] skipping proxyClientCA CA generation: /home/burak/.minikube/proxy-client-ca.key
I1011 13:49:19.753087  311659 certs.go:302] generating minikube-user signed cert: /home/burak/.minikube/profiles/minikube/client.key
I1011 13:49:19.753092  311659 crypto.go:68] Generating cert /home/burak/.minikube/profiles/minikube/client.crt with IP's: []
I1011 13:49:19.841803  311659 crypto.go:156] Writing cert to /home/burak/.minikube/profiles/minikube/client.crt ...
I1011 13:49:19.841830  311659 lock.go:35] WriteFile acquiring /home/burak/.minikube/profiles/minikube/client.crt: {Name:mkb04e94279115646a7ce6a6dcb85b54c6409ddf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:19.842117  311659 crypto.go:164] Writing key to /home/burak/.minikube/profiles/minikube/client.key ...
I1011 13:49:19.842125  311659 lock.go:35] WriteFile acquiring /home/burak/.minikube/profiles/minikube/client.key: {Name:mk20dff7fc55bb94385e71e52e5c5955d48d0ecf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:19.842212  311659 certs.go:302] generating minikube signed cert: /home/burak/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1011 13:49:19.842219  311659 crypto.go:68] Generating cert /home/burak/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1011 13:49:19.976260  311659 crypto.go:156] Writing cert to /home/burak/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I1011 13:49:19.976270  311659 lock.go:35] WriteFile acquiring /home/burak/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk923b664b29fdfe94f5885aabcea2630028a5d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:19.976422  311659 crypto.go:164] Writing key to /home/burak/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I1011 13:49:19.976426  311659 lock.go:35] WriteFile acquiring /home/burak/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk281af1801ff4c727d6232c3ed6ba33c2c92bd1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:19.976510  311659 certs.go:320] copying /home/burak/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /home/burak/.minikube/profiles/minikube/apiserver.crt
I1011 13:49:19.976545  311659 certs.go:324] copying /home/burak/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /home/burak/.minikube/profiles/minikube/apiserver.key
I1011 13:49:19.976575  311659 certs.go:302] generating aggregator signed cert: /home/burak/.minikube/profiles/minikube/proxy-client.key
I1011 13:49:19.976581  311659 crypto.go:68] Generating cert /home/burak/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1011 13:49:20.061782  311659 crypto.go:156] Writing cert to /home/burak/.minikube/profiles/minikube/proxy-client.crt ...
I1011 13:49:20.061792  311659 lock.go:35] WriteFile acquiring /home/burak/.minikube/profiles/minikube/proxy-client.crt: {Name:mk23c896ffc8e097ab1adc8fc455b698bf858b16 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:20.061918  311659 crypto.go:164] Writing key to /home/burak/.minikube/profiles/minikube/proxy-client.key ...
I1011 13:49:20.061923  311659 lock.go:35] WriteFile acquiring /home/burak/.minikube/profiles/minikube/proxy-client.key: {Name:mke4256287c82e417ac954def15824dbc78d3455 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:20.062029  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca-key.pem (1679 bytes)
I1011 13:49:20.062049  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca.pem (1074 bytes)
I1011 13:49:20.062062  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/cert.pem (1119 bytes)
I1011 13:49:20.062073  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/key.pem (1679 bytes)
I1011 13:49:20.062450  311659 ssh_runner.go:362] scp /home/burak/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1011 13:49:20.075565  311659 ssh_runner.go:362] scp /home/burak/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1011 13:49:20.088080  311659 ssh_runner.go:362] scp /home/burak/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1011 13:49:20.101045  311659 ssh_runner.go:362] scp /home/burak/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1011 13:49:20.116440  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1011 13:49:20.131562  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1011 13:49:20.145876  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1011 13:49:20.158618  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1011 13:49:20.171174  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1011 13:49:20.184030  311659 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1011 13:49:20.195250  311659 ssh_runner.go:195] Run: openssl version
I1011 13:49:20.200111  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1011 13:49:20.205749  311659 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1011 13:49:20.208010  311659 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Oct  5 02:47 /usr/share/ca-certificates/minikubeCA.pem
I1011 13:49:20.208040  311659 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1011 13:49:20.211636  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1011 13:49:20.218181  311659 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1011 13:49:20.218250  311659 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1011 13:49:20.248606  311659 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1011 13:49:20.254490  311659 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1011 13:49:20.260319  311659 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I1011 13:49:20.260347  311659 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1011 13:49:20.265125  311659 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1011 13:49:20.265144  311659 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1011 13:49:20.307155  311659 kubeadm.go:317] [init] Using Kubernetes version: v1.25.0
I1011 13:49:20.307296  311659 kubeadm.go:317] [preflight] Running pre-flight checks
I1011 13:49:20.343384  311659 kubeadm.go:317] [preflight] The system verification failed. Printing the output from the verification:
I1011 13:49:20.343460  311659 kubeadm.go:317] [0;37mKERNEL_VERSION[0m: [0;32m5.15.0-50-generic[0m
I1011 13:49:20.343497  311659 kubeadm.go:317] [0;37mOS[0m: [0;32mLinux[0m
I1011 13:49:20.343553  311659 kubeadm.go:317] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I1011 13:49:20.343604  311659 kubeadm.go:317] [0;37mCGROUPS_CPUACCT[0m: [0;32menabled[0m
I1011 13:49:20.343641  311659 kubeadm.go:317] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I1011 13:49:20.343675  311659 kubeadm.go:317] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I1011 13:49:20.343712  311659 kubeadm.go:317] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I1011 13:49:20.343752  311659 kubeadm.go:317] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I1011 13:49:20.343799  311659 kubeadm.go:317] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I1011 13:49:20.343845  311659 kubeadm.go:317] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I1011 13:49:20.343881  311659 kubeadm.go:317] [0;37mCGROUPS_BLKIO[0m: [0;32menabled[0m
I1011 13:49:20.387543  311659 kubeadm.go:317] [preflight] Pulling images required for setting up a Kubernetes cluster
I1011 13:49:20.387644  311659 kubeadm.go:317] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1011 13:49:20.387744  311659 kubeadm.go:317] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1011 13:49:20.482574  311659 kubeadm.go:317] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1011 13:49:20.485664  311659 out.go:204]     ‚ñ™ Generating certificates and keys ...
I1011 13:49:20.485790  311659 kubeadm.go:317] [certs] Using existing ca certificate authority
I1011 13:49:20.485853  311659 kubeadm.go:317] [certs] Using existing apiserver certificate and key on disk
I1011 13:49:20.824531  311659 kubeadm.go:317] [certs] Generating "apiserver-kubelet-client" certificate and key
I1011 13:49:20.868212  311659 kubeadm.go:317] [certs] Generating "front-proxy-ca" certificate and key
I1011 13:49:21.146627  311659 kubeadm.go:317] [certs] Generating "front-proxy-client" certificate and key
I1011 13:49:21.507408  311659 kubeadm.go:317] [certs] Generating "etcd/ca" certificate and key
I1011 13:49:21.593417  311659 kubeadm.go:317] [certs] Generating "etcd/server" certificate and key
I1011 13:49:21.593549  311659 kubeadm.go:317] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1011 13:49:21.716455  311659 kubeadm.go:317] [certs] Generating "etcd/peer" certificate and key
I1011 13:49:21.716673  311659 kubeadm.go:317] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1011 13:49:21.921097  311659 kubeadm.go:317] [certs] Generating "etcd/healthcheck-client" certificate and key
I1011 13:49:22.350484  311659 kubeadm.go:317] [certs] Generating "apiserver-etcd-client" certificate and key
I1011 13:49:22.490949  311659 kubeadm.go:317] [certs] Generating "sa" key and public key
I1011 13:49:22.491024  311659 kubeadm.go:317] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1011 13:49:22.535622  311659 kubeadm.go:317] [kubeconfig] Writing "admin.conf" kubeconfig file
I1011 13:49:22.601781  311659 kubeadm.go:317] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1011 13:49:22.778723  311659 kubeadm.go:317] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1011 13:49:22.881373  311659 kubeadm.go:317] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1011 13:49:22.890184  311659 kubeadm.go:317] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1011 13:49:22.890786  311659 kubeadm.go:317] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1011 13:49:22.890844  311659 kubeadm.go:317] [kubelet-start] Starting the kubelet
I1011 13:49:22.967459  311659 kubeadm.go:317] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1011 13:49:22.971052  311659 out.go:204]     ‚ñ™ Booting up control plane ...
I1011 13:49:22.971192  311659 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1011 13:49:22.971264  311659 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1011 13:49:22.972146  311659 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1011 13:49:22.973005  311659 kubeadm.go:317] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1011 13:49:22.974852  311659 kubeadm.go:317] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1011 13:49:35.986317  311659 kubeadm.go:317] [apiclient] All control plane components are healthy after 13.005432 seconds
I1011 13:49:35.986790  311659 kubeadm.go:317] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1011 13:49:36.009185  311659 kubeadm.go:317] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1011 13:49:36.558171  311659 kubeadm.go:317] [upload-certs] Skipping phase. Please see --upload-certs
I1011 13:49:36.558849  311659 kubeadm.go:317] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1011 13:49:37.081789  311659 kubeadm.go:317] [bootstrap-token] Using token: 0ex7qt.8qinloxtci447bvf
I1011 13:49:37.085356  311659 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I1011 13:49:37.086016  311659 kubeadm.go:317] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1011 13:49:37.100968  311659 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1011 13:49:37.116749  311659 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1011 13:49:37.128030  311659 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1011 13:49:37.138661  311659 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1011 13:49:37.153340  311659 kubeadm.go:317] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1011 13:49:37.169488  311659 kubeadm.go:317] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1011 13:49:37.385906  311659 kubeadm.go:317] [addons] Applied essential addon: CoreDNS
I1011 13:49:37.510812  311659 kubeadm.go:317] [addons] Applied essential addon: kube-proxy
I1011 13:49:37.515385  311659 kubeadm.go:317] 
I1011 13:49:37.515657  311659 kubeadm.go:317] Your Kubernetes control-plane has initialized successfully!
I1011 13:49:37.515690  311659 kubeadm.go:317] 
I1011 13:49:37.516021  311659 kubeadm.go:317] To start using your cluster, you need to run the following as a regular user:
I1011 13:49:37.516039  311659 kubeadm.go:317] 
I1011 13:49:37.516144  311659 kubeadm.go:317]   mkdir -p $HOME/.kube
I1011 13:49:37.516582  311659 kubeadm.go:317]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1011 13:49:37.516787  311659 kubeadm.go:317]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1011 13:49:37.516805  311659 kubeadm.go:317] 
I1011 13:49:37.517032  311659 kubeadm.go:317] Alternatively, if you are the root user, you can run:
I1011 13:49:37.517049  311659 kubeadm.go:317] 
I1011 13:49:37.517256  311659 kubeadm.go:317]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1011 13:49:37.517272  311659 kubeadm.go:317] 
I1011 13:49:37.517496  311659 kubeadm.go:317] You should now deploy a pod network to the cluster.
I1011 13:49:37.517851  311659 kubeadm.go:317] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1011 13:49:37.518126  311659 kubeadm.go:317]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1011 13:49:37.518144  311659 kubeadm.go:317] 
I1011 13:49:37.518500  311659 kubeadm.go:317] You can now join any number of control-plane nodes by copying certificate authorities
I1011 13:49:37.518827  311659 kubeadm.go:317] and service account keys on each node and then running the following as root:
I1011 13:49:37.518846  311659 kubeadm.go:317] 
I1011 13:49:37.519239  311659 kubeadm.go:317]   kubeadm join control-plane.minikube.internal:8443 --token 0ex7qt.8qinloxtci447bvf \
I1011 13:49:37.519705  311659 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 \
I1011 13:49:37.519800  311659 kubeadm.go:317] 	--control-plane 
I1011 13:49:37.519819  311659 kubeadm.go:317] 
I1011 13:49:37.520226  311659 kubeadm.go:317] Then you can join any number of worker nodes by running the following on each as root:
I1011 13:49:37.520245  311659 kubeadm.go:317] 
I1011 13:49:37.520605  311659 kubeadm.go:317] kubeadm join control-plane.minikube.internal:8443 --token 0ex7qt.8qinloxtci447bvf \
I1011 13:49:37.521081  311659 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 
I1011 13:49:37.547289  311659 kubeadm.go:317] W1011 10:49:20.299699    1107 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I1011 13:49:37.547523  311659 kubeadm.go:317] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I1011 13:49:37.547903  311659 kubeadm.go:317] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-50-generic\n", err: exit status 1
I1011 13:49:37.548075  311659 kubeadm.go:317] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1011 13:49:37.548094  311659 cni.go:95] Creating CNI manager for ""
I1011 13:49:37.548101  311659 cni.go:156] 1 nodes found, recommending kindnet
I1011 13:49:37.551458  311659 out.go:177] üîó  Configuring CNI (Container Networking Interface) ...
I1011 13:49:37.560609  311659 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1011 13:49:37.629618  311659 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.25.0/kubectl ...
I1011 13:49:37.629637  311659 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I1011 13:49:37.720716  311659 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1011 13:49:39.543234  311659 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.25.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (1.822489643s)
I1011 13:49:39.543258  311659 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1011 13:49:39.543312  311659 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1011 13:49:39.543319  311659 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl label nodes minikube.k8s.io/version=v1.27.0 minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2022_10_11T13_49_39_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1011 13:49:39.615414  311659 ops.go:34] apiserver oom_adj: -16
I1011 13:49:39.615419  311659 kubeadm.go:1067] duration metric: took 72.146201ms to wait for elevateKubeSystemPrivileges.
I1011 13:49:39.615428  311659 kubeadm.go:398] StartCluster complete in 19.397255497s
I1011 13:49:39.615437  311659 settings.go:142] acquiring lock: {Name:mk0a03cb60e659e49f8b37aa928da007a3de6e9b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:39.615501  311659 settings.go:150] Updating kubeconfig:  /home/burak/.kube/config
I1011 13:49:39.616752  311659 lock.go:35] WriteFile acquiring /home/burak/.kube/config: {Name:mk1540fe7448652c51043b5e723566f40e9109fa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1011 13:49:40.131891  311659 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1011 13:49:40.131980  311659 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1011 13:49:40.135391  311659 out.go:177] üîé  Verifying Kubernetes components...
I1011 13:49:40.132068  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1011 13:49:40.132096  311659 addons.go:412] enableAddons start: toEnable=map[], additional=[]
I1011 13:49:40.132585  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:49:40.138653  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1011 13:49:40.138772  311659 cache.go:107] acquiring lock: {Name:mk9d245b52f2a15608b7182fe894299c8834db79 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:49:40.139366  311659 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1011 13:49:40.139481  311659 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1011 13:49:40.139540  311659 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1011 13:49:40.140601  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:40.141015  311659 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W1011 13:49:40.141040  311659 addons.go:162] addon storage-provisioner should already be in state true
I1011 13:49:40.141182  311659 host.go:66] Checking if "minikube" exists ...
I1011 13:49:40.142588  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:40.155126  311659 cache.go:115] /home/burak/.minikube/cache/images/amd64/alpine_latest exists
I1011 13:49:40.155217  311659 cache.go:96] cache image "alpine:latest" -> "/home/burak/.minikube/cache/images/amd64/alpine_latest" took 16.462174ms
I1011 13:49:40.155248  311659 cache.go:80] save to tar file alpine:latest -> /home/burak/.minikube/cache/images/amd64/alpine_latest succeeded
I1011 13:49:40.155312  311659 cache.go:87] Successfully saved all images to host disk.
I1011 13:49:40.155779  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:49:40.156659  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:40.272248  311659 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1011 13:49:40.272259  311659 addons.go:162] addon default-storageclass should already be in state true
I1011 13:49:40.272281  311659 host.go:66] Checking if "minikube" exists ...
I1011 13:49:40.272680  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:40.279737  311659 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1011 13:49:40.278957  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:49:40.282634  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:40.282825  311659 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1011 13:49:40.282832  311659 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1011 13:49:40.282859  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:40.314034  311659 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I1011 13:49:40.314045  311659 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1011 13:49:40.314093  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:40.314182  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1011 13:49:40.315936  311659 api_server.go:51] waiting for apiserver process to appear ...
I1011 13:49:40.316012  311659 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1011 13:49:40.343766  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:40.347650  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:40.365075  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:40.456628  311659 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1011 13:49:40.508038  311659 api_server.go:71] duration metric: took 376.011531ms to wait for apiserver process to appear ...
I1011 13:49:40.508052  311659 api_server.go:87] waiting for apiserver healthz status ...
I1011 13:49:40.508060  311659 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1011 13:49:40.508293  311659 start.go:810] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS
I1011 13:49:40.513339  311659 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I1011 13:49:40.516544  311659 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1011 13:49:40.516750  311659 api_server.go:140] control plane version: v1.25.0
I1011 13:49:40.516760  311659 api_server.go:130] duration metric: took 8.703987ms to wait for apiserver health ...
I1011 13:49:40.516766  311659 system_pods.go:43] waiting for kube-system pods to appear ...
I1011 13:49:40.523851  311659 system_pods.go:59] 4 kube-system pods found
I1011 13:49:40.523865  311659 system_pods.go:61] "etcd-minikube" [cc552d83-d2b1-413c-a2d6-1eb71a6e4126] Running
I1011 13:49:40.523870  311659 system_pods.go:61] "kube-apiserver-minikube" [259a8bab-b64f-443d-85c2-323394e734dc] Running
I1011 13:49:40.523874  311659 system_pods.go:61] "kube-controller-manager-minikube" [584ac1ce-d0c5-415f-b462-f35eaf6d2db4] Running
I1011 13:49:40.523878  311659 system_pods.go:61] "kube-scheduler-minikube" [01508e20-85f4-45e3-b783-026eb67355fc] Running
I1011 13:49:40.523882  311659 system_pods.go:74] duration metric: took 7.107157ms to wait for pod list to return data ...
I1011 13:49:40.523889  311659 kubeadm.go:573] duration metric: took 391.868964ms to wait for : map[apiserver:true system_pods:true] ...
I1011 13:49:40.523899  311659 node_conditions.go:102] verifying NodePressure condition ...
I1011 13:49:40.527001  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:49:40.527015  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:49:40.527023  311659 node_conditions.go:105] duration metric: took 3.120733ms to run NodePressure ...
I1011 13:49:40.527034  311659 start.go:216] waiting for startup goroutines ...
I1011 13:49:40.843292  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:49:40.843302  311659 docker.go:617] alpine:latest wasn't preloaded
I1011 13:49:40.843306  311659 cache_images.go:88] LoadImages start: [alpine:latest]
I1011 13:49:40.844302  311659 image.go:134] retrieving image: alpine:latest
I1011 13:49:40.844311  311659 image.go:140] checking repository: index.docker.io/library/alpine
I1011 13:49:40.856349  311659 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1011 13:49:40.859165  311659 addons.go:414] enableAddons completed in 727.064488ms
I1011 13:49:42.708291  311659 image.go:144] canonical name: docker.io/library/alpine:latest
I1011 13:49:42.710063  311659 image.go:177] daemon lookup for alpine:latest: Error: No such image: alpine:latest
I1011 13:49:43.819793  311659 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} alpine:latest
I1011 13:49:43.869162  311659 cache_images.go:116] "alpine:latest" needs transfer: "alpine:latest" does not exist at hash "9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5" in container runtime
I1011 13:49:43.869204  311659 docker.go:292] Removing image: alpine:latest
I1011 13:49:43.869240  311659 ssh_runner.go:195] Run: docker rmi alpine:latest
I1011 13:49:43.890691  311659 cache_images.go:286] Loading image from: /home/burak/.minikube/cache/images/amd64/alpine_latest
I1011 13:49:43.890740  311659 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest
I1011 13:49:43.892800  311659 ssh_runner.go:352] existence check for /var/lib/minikube/images/alpine_latest: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/alpine_latest': No such file or directory
I1011 13:49:43.892817  311659 ssh_runner.go:362] scp /home/burak/.minikube/cache/images/amd64/alpine_latest --> /var/lib/minikube/images/alpine_latest (2810880 bytes)
I1011 13:49:43.913645  311659 docker.go:259] Loading image: /var/lib/minikube/images/alpine_latest
I1011 13:49:43.913659  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/alpine_latest | docker load"
I1011 13:49:44.138299  311659 cache_images.go:315] Transferred and loaded /home/burak/.minikube/cache/images/amd64/alpine_latest from cache
I1011 13:49:44.138346  311659 cache_images.go:123] Successfully loaded all cached images
I1011 13:49:44.138359  311659 cache_images.go:92] LoadImages completed in 3.295040882s
I1011 13:49:44.138387  311659 cache_images.go:262] succeeded pushing to: minikube
I1011 13:49:44.138397  311659 cache_images.go:263] failed pushing to: 
I1011 13:49:44.142121  311659 out.go:177] 
I1011 13:49:44.145627  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:49:44.145949  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:49:44.149936  311659 out.go:177] üëç  Starting worker node minikube-m02 in cluster minikube
I1011 13:49:44.153125  311659 cache.go:120] Beginning downloading kic base image for docker with docker
I1011 13:49:44.156468  311659 out.go:177] üöú  Pulling base image ...
I1011 13:49:44.164613  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:49:44.164666  311659 cache.go:57] Caching tarball of preloaded images
I1011 13:49:44.164753  311659 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I1011 13:49:44.164991  311659 preload.go:174] Found /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1011 13:49:44.165073  311659 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.0 on docker
I1011 13:49:44.165373  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:49:44.279094  311659 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I1011 13:49:44.279123  311659 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I1011 13:49:44.279159  311659 cache.go:208] Successfully downloaded all kic artifacts
I1011 13:49:44.279212  311659 start.go:364] acquiring machines lock for minikube-m02: {Name:mkb9bae71bb7bd3de80ab3921df7a2dc50fd5d2c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:49:44.279413  311659 start.go:368] acquired machines lock for "minikube-m02" in 171.944¬µs
I1011 13:49:44.279451  311659 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name:m02 IP: Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:49:44.279616  311659 start.go:125] createHost starting for "m02" (driver="docker")
I1011 13:49:44.283113  311659 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I1011 13:49:44.283431  311659 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1011 13:49:44.283472  311659 client.go:168] LocalClient.Create starting
I1011 13:49:44.283614  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/ca.pem
I1011 13:49:44.283697  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:49:44.283745  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:49:44.283918  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/cert.pem
I1011 13:49:44.283988  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:49:44.284034  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:49:44.285343  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:49:44.371976  311659 network_create.go:76] Found existing network {name:minikube subnet:0xc000ca2930 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I1011 13:49:44.374883  311659 kic.go:106] calculated static IP "192.168.49.3" for the "minikube-m02" container
I1011 13:49:44.375033  311659 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1011 13:49:44.440815  311659 cli_runner.go:164] Run: docker volume create minikube-m02 --label name.minikube.sigs.k8s.io=minikube-m02 --label created_by.minikube.sigs.k8s.io=true
I1011 13:49:44.463414  311659 oci.go:103] Successfully created a docker volume minikube-m02
I1011 13:49:44.463455  311659 cli_runner.go:164] Run: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I1011 13:49:45.102762  311659 oci.go:107] Successfully prepared a docker volume minikube-m02
I1011 13:49:45.102782  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:49:45.102801  311659 kic.go:179] Starting extracting preloaded images to volume ...
I1011 13:49:45.102848  311659 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
I1011 13:49:47.537629  311659 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (2.434712019s)
I1011 13:49:47.537671  311659 kic.go:188] duration metric: took 2.434861 seconds to extract preloaded images to volume
W1011 13:49:47.538289  311659 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1011 13:49:47.538494  311659 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1011 13:49:47.758773  311659 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I1011 13:49:48.155222  311659 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Running}}
I1011 13:49:48.178904  311659 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1011 13:49:48.203600  311659 cli_runner.go:164] Run: docker exec minikube-m02 stat /var/lib/dpkg/alternatives/iptables
I1011 13:49:48.319643  311659 oci.go:144] the created container "minikube-m02" has a running status.
I1011 13:49:48.319811  311659 kic.go:210] Creating ssh key for kic: /home/burak/.minikube/machines/minikube-m02/id_rsa...
I1011 13:49:48.517023  311659 kic_runner.go:191] docker (temp): /home/burak/.minikube/machines/minikube-m02/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1011 13:49:48.723941  311659 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1011 13:49:48.780570  311659 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1011 13:49:48.780580  311659 kic_runner.go:114] Args: [docker exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I1011 13:49:48.856867  311659 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1011 13:49:48.884397  311659 machine.go:88] provisioning docker machine ...
I1011 13:49:48.884683  311659 ubuntu.go:169] provisioning hostname "minikube-m02"
I1011 13:49:48.884722  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:48.908718  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:48.908852  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49182 <nil> <nil>}
I1011 13:49:48.908867  311659 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I1011 13:49:49.066023  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube-m02

I1011 13:49:49.066074  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:49.106000  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:49.106101  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49182 <nil> <nil>}
I1011 13:49:49.106111  311659 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I1011 13:49:49.240480  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1011 13:49:49.240502  311659 ubuntu.go:175] set auth options {CertDir:/home/burak/.minikube CaCertPath:/home/burak/.minikube/certs/ca.pem CaPrivateKeyPath:/home/burak/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/burak/.minikube/machines/server.pem ServerKeyPath:/home/burak/.minikube/machines/server-key.pem ClientKeyPath:/home/burak/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/burak/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/burak/.minikube}
I1011 13:49:49.240517  311659 ubuntu.go:177] setting up certificates
I1011 13:49:49.240523  311659 provision.go:83] configureAuth start
I1011 13:49:49.240558  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1011 13:49:49.262018  311659 provision.go:138] copyHostCerts
I1011 13:49:49.262052  311659 exec_runner.go:144] found /home/burak/.minikube/ca.pem, removing ...
I1011 13:49:49.262057  311659 exec_runner.go:207] rm: /home/burak/.minikube/ca.pem
I1011 13:49:49.262101  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/ca.pem --> /home/burak/.minikube/ca.pem (1074 bytes)
I1011 13:49:49.262155  311659 exec_runner.go:144] found /home/burak/.minikube/cert.pem, removing ...
I1011 13:49:49.262157  311659 exec_runner.go:207] rm: /home/burak/.minikube/cert.pem
I1011 13:49:49.262180  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/cert.pem --> /home/burak/.minikube/cert.pem (1119 bytes)
I1011 13:49:49.262219  311659 exec_runner.go:144] found /home/burak/.minikube/key.pem, removing ...
I1011 13:49:49.262221  311659 exec_runner.go:207] rm: /home/burak/.minikube/key.pem
I1011 13:49:49.262240  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/key.pem --> /home/burak/.minikube/key.pem (1679 bytes)
I1011 13:49:49.262273  311659 provision.go:112] generating server cert: /home/burak/.minikube/machines/server.pem ca-key=/home/burak/.minikube/certs/ca.pem private-key=/home/burak/.minikube/certs/ca-key.pem org=burak.minikube-m02 san=[192.168.49.3 127.0.0.1 localhost 127.0.0.1 minikube minikube-m02]
I1011 13:49:49.396540  311659 provision.go:172] copyRemoteCerts
I1011 13:49:49.396576  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1011 13:49:49.396598  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:49.419425  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:49:49.514862  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I1011 13:49:49.531193  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1011 13:49:49.547729  311659 ssh_runner.go:362] scp /home/burak/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1011 13:49:49.561806  311659 provision.go:86] duration metric: configureAuth took 321.275537ms
I1011 13:49:49.561816  311659 ubuntu.go:193] setting minikube options for container-runtime
I1011 13:49:49.561931  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:49:49.561967  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:49.581049  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:49.581149  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49182 <nil> <nil>}
I1011 13:49:49.581156  311659 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1011 13:49:49.711502  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1011 13:49:49.711518  311659 ubuntu.go:71] root file system type: overlay
I1011 13:49:49.712711  311659 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1011 13:49:49.712877  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:49.776578  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:49.776685  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49182 <nil> <nil>}
I1011 13:49:49.776740  311659 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1011 13:49:49.982957  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1011 13:49:49.983514  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:50.059452  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:49:50.059587  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49182 <nil> <nil>}
I1011 13:49:50.059601  311659 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1011 13:49:50.835865  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-10-11 10:49:49.961915762 +0000
@@ -1,30 +1,33 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=NO_PROXY=192.168.49.2
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +35,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1011 13:49:50.835880  311659 machine.go:91] provisioned docker machine in 1.951473123s
I1011 13:49:50.835887  311659 client.go:171] LocalClient.Create took 6.552409943s
I1011 13:49:50.835896  311659 start.go:167] duration metric: libmachine.API.Create for "minikube" took 6.552476522s
I1011 13:49:50.835901  311659 start.go:300] post-start starting for "minikube-m02" (driver="docker")
I1011 13:49:50.835907  311659 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1011 13:49:50.835949  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1011 13:49:50.835982  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:50.864311  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:49:50.958816  311659 ssh_runner.go:195] Run: cat /etc/os-release
I1011 13:49:50.963376  311659 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1011 13:49:50.963397  311659 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1011 13:49:50.963410  311659 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1011 13:49:50.963418  311659 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1011 13:49:50.963431  311659 filesync.go:126] Scanning /home/burak/.minikube/addons for local assets ...
I1011 13:49:50.963498  311659 filesync.go:126] Scanning /home/burak/.minikube/files for local assets ...
I1011 13:49:50.963533  311659 start.go:303] post-start completed in 127.622992ms
I1011 13:49:50.964017  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1011 13:49:51.043596  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:49:51.044099  311659 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1011 13:49:51.044176  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:51.084454  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:49:51.173623  311659 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1011 13:49:51.177809  311659 start.go:128] duration metric: createHost completed in 6.89818242s
I1011 13:49:51.177821  311659 start.go:83] releasing machines lock for "minikube-m02", held for 6.898396448s
I1011 13:49:51.177893  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1011 13:49:51.208721  311659 out.go:177] üåê  Found network options:
I1011 13:49:51.211783  311659 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2
W1011 13:49:51.214656  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:49:51.214701  311659 proxy.go:119] fail to check proxy env: Error ip not in block
I1011 13:49:51.214845  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1011 13:49:51.214889  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:51.215079  311659 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1011 13:49:51.215146  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:51.258702  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:49:51.265636  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:49:51.541776  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (234 bytes)
I1011 13:49:51.580831  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:49:51.747023  311659 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1011 13:49:51.807545  311659 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1011 13:49:51.815229  311659 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1011 13:49:51.815266  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1011 13:49:51.822282  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1011 13:49:51.833449  311659 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1011 13:49:51.907939  311659 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1011 13:49:51.975300  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:49:52.047933  311659 ssh_runner.go:195] Run: sudo systemctl restart docker
I1011 13:49:52.208126  311659 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1011 13:49:52.280393  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:49:52.353876  311659 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1011 13:49:52.360623  311659 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1011 13:49:52.360661  311659 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1011 13:49:52.362787  311659 start.go:471] Will wait 60s for crictl version
I1011 13:49:52.362817  311659 ssh_runner.go:195] Run: sudo crictl version
I1011 13:49:52.384856  311659 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1011 13:49:52.384890  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:49:52.414697  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:49:52.445394  311659 out.go:204] üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
I1011 13:49:52.448121  311659 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I1011 13:49:52.450968  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:49:52.479957  311659 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1011 13:49:52.483059  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:49:52.490420  311659 certs.go:54] Setting up /home/burak/.minikube/profiles/minikube for IP: 192.168.49.3
I1011 13:49:52.490491  311659 certs.go:182] skipping minikubeCA CA generation: /home/burak/.minikube/ca.key
I1011 13:49:52.490515  311659 certs.go:182] skipping proxyClientCA CA generation: /home/burak/.minikube/proxy-client-ca.key
I1011 13:49:52.490555  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca-key.pem (1679 bytes)
I1011 13:49:52.490571  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca.pem (1074 bytes)
I1011 13:49:52.490585  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/cert.pem (1119 bytes)
I1011 13:49:52.490597  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/key.pem (1679 bytes)
I1011 13:49:52.490858  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1011 13:49:52.505883  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1011 13:49:52.519780  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1011 13:49:52.533610  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1011 13:49:52.546229  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1011 13:49:52.559536  311659 ssh_runner.go:195] Run: openssl version
I1011 13:49:52.563097  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1011 13:49:52.568728  311659 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1011 13:49:52.571123  311659 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Oct  5 02:47 /usr/share/ca-certificates/minikubeCA.pem
I1011 13:49:52.571163  311659 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1011 13:49:52.574766  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1011 13:49:52.579964  311659 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1011 13:49:52.722357  311659 cni.go:95] Creating CNI manager for ""
I1011 13:49:52.722365  311659 cni.go:156] 2 nodes found, recommending kindnet
I1011 13:49:52.722374  311659 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1011 13:49:52.722384  311659 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.3 APIServerPort:8443 KubernetesVersion:v1.25.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m02 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.3 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:true}
I1011 13:49:52.722647  311659 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube-m02"
  kubeletExtraArgs:
    node-ip: 192.168.49.3
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
resolvConf: /etc/kubelet-resolv.conf
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1011 13:49:52.722683  311659 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube-m02 --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.3 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1011 13:49:52.722714  311659 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.0
I1011 13:49:52.727916  311659 binaries.go:44] Found k8s binaries, skipping transfer
I1011 13:49:52.727949  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1011 13:49:52.734722  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (474 bytes)
I1011 13:49:52.746240  311659 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1011 13:49:52.755896  311659 ssh_runner.go:195] Run: sudo cp /etc/resolv.conf /etc/kubelet-resolv.conf
I1011 13:49:52.761142  311659 ssh_runner.go:195] Run: sudo sed -i -e "s/^search .$//" /etc/kubelet-resolv.conf
I1011 13:49:52.766562  311659 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1011 13:49:52.768648  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:49:52.775180  311659 host.go:66] Checking if "minikube" exists ...
I1011 13:49:52.775357  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:49:52.775368  311659 start.go:285] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1011 13:49:52.775401  311659 cache.go:107] acquiring lock: {Name:mk9d245b52f2a15608b7182fe894299c8834db79 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:49:52.775423  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I1011 13:49:52.775459  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:52.775480  311659 cache.go:115] /home/burak/.minikube/cache/images/amd64/alpine_latest exists
I1011 13:49:52.775493  311659 cache.go:96] cache image "alpine:latest" -> "/home/burak/.minikube/cache/images/amd64/alpine_latest" took 96.793¬µs
I1011 13:49:52.775499  311659 cache.go:80] save to tar file alpine:latest -> /home/burak/.minikube/cache/images/amd64/alpine_latest succeeded
I1011 13:49:52.775505  311659 cache.go:87] Successfully saved all images to host disk.
I1011 13:49:52.775592  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:49:52.775738  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:49:52.796651  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:49:52.796683  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:49:52.797332  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:52.817273  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:49:52.980025  311659 start.go:306] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:49:52.980053  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token c3cier.g2n4g731rr7k6ycl --discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02"
I1011 13:49:52.980070  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
alpine:latest
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:49:52.980077  311659 cache_images.go:84] Images are preloaded, skipping loading
I1011 13:49:52.980288  311659 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1011 13:49:52.999716  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:49:52.999745  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:49:53.019158  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:49:58.879834  311659 ssh_runner.go:235] Completed: docker images --format {{.Repository}}:{{.Tag}}: (5.880089814s)
I1011 13:49:58.879868  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:49:58.879878  311659 docker.go:617] alpine:latest wasn't preloaded
I1011 13:49:58.879886  311659 cache_images.go:88] LoadImages start: [alpine:latest]
I1011 13:49:58.880150  311659 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token c3cier.g2n4g731rr7k6ycl --discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02": (5.900076102s)
I1011 13:49:58.880378  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1011 13:49:58.881427  311659 image.go:134] retrieving image: alpine:latest
I1011 13:49:58.881447  311659 image.go:140] checking repository: index.docker.io/library/alpine
I1011 13:49:59.092905  311659 start.go:287] JoinCluster complete in 6.317534314s
I1011 13:49:59.092915  311659 cni.go:95] Creating CNI manager for ""
I1011 13:49:59.092919  311659 cni.go:156] 2 nodes found, recommending kindnet
I1011 13:49:59.092958  311659 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1011 13:49:59.095901  311659 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.25.0/kubectl ...
I1011 13:49:59.095908  311659 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I1011 13:49:59.106930  311659 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1011 13:49:59.268587  311659 start.go:211] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:49:59.271476  311659 out.go:177] üîé  Verifying Kubernetes components...
I1011 13:49:59.276813  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1011 13:49:59.285948  311659 kubeadm.go:573] duration metric: took 17.339267ms to wait for : map[apiserver:true system_pods:true] ...
I1011 13:49:59.285959  311659 node_conditions.go:102] verifying NodePressure condition ...
I1011 13:49:59.287828  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:49:59.287837  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:49:59.287843  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:49:59.287848  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:49:59.287851  311659 node_conditions.go:105] duration metric: took 1.888837ms to run NodePressure ...
I1011 13:49:59.287857  311659 start.go:216] waiting for startup goroutines ...
I1011 13:50:00.670573  311659 image.go:144] canonical name: docker.io/library/alpine:latest
I1011 13:50:00.673210  311659 image.go:177] daemon lookup for alpine:latest: Error: No such image: alpine:latest
I1011 13:50:01.593325  311659 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} alpine:latest
I1011 13:50:01.652524  311659 cache_images.go:116] "alpine:latest" needs transfer: "alpine:latest" does not exist at hash "9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5" in container runtime
I1011 13:50:01.652816  311659 docker.go:292] Removing image: alpine:latest
I1011 13:50:01.652860  311659 ssh_runner.go:195] Run: docker rmi alpine:latest
I1011 13:50:01.675546  311659 cache_images.go:286] Loading image from: /home/burak/.minikube/cache/images/amd64/alpine_latest
I1011 13:50:01.675602  311659 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest
I1011 13:50:01.677649  311659 ssh_runner.go:352] existence check for /var/lib/minikube/images/alpine_latest: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/alpine_latest': No such file or directory
I1011 13:50:01.677661  311659 ssh_runner.go:362] scp /home/burak/.minikube/cache/images/amd64/alpine_latest --> /var/lib/minikube/images/alpine_latest (2810880 bytes)
I1011 13:50:01.699845  311659 docker.go:259] Loading image: /var/lib/minikube/images/alpine_latest
I1011 13:50:01.699858  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/alpine_latest | docker load"
I1011 13:50:01.884730  311659 cache_images.go:315] Transferred and loaded /home/burak/.minikube/cache/images/amd64/alpine_latest from cache
I1011 13:50:01.884781  311659 cache_images.go:123] Successfully loaded all cached images
I1011 13:50:01.884795  311659 cache_images.go:92] LoadImages completed in 3.004894025s
I1011 13:50:01.884823  311659 cache_images.go:262] succeeded pushing to: minikube minikube-m02
I1011 13:50:01.884835  311659 cache_images.go:263] failed pushing to: 
I1011 13:50:01.888527  311659 out.go:177] 
I1011 13:50:01.892160  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:01.892447  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:50:01.896692  311659 out.go:177] üëç  Starting worker node minikube-m03 in cluster minikube
I1011 13:50:01.899793  311659 cache.go:120] Beginning downloading kic base image for docker with docker
I1011 13:50:01.903033  311659 out.go:177] üöú  Pulling base image ...
I1011 13:50:01.908714  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:50:01.908768  311659 cache.go:57] Caching tarball of preloaded images
I1011 13:50:01.908897  311659 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I1011 13:50:01.909567  311659 preload.go:174] Found /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1011 13:50:01.909620  311659 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.0 on docker
I1011 13:50:01.910008  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:50:01.986030  311659 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I1011 13:50:01.986049  311659 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I1011 13:50:01.986064  311659 cache.go:208] Successfully downloaded all kic artifacts
I1011 13:50:01.986095  311659 start.go:364] acquiring machines lock for minikube-m03: {Name:mk35f56c6f60fdad2d7bbfe40dda54615245ab18 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:50:01.986220  311659 start.go:368] acquired machines lock for "minikube-m03" in 107.897¬µs
I1011 13:50:01.986238  311659 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP: Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name:m03 IP: Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:50:01.986317  311659 start.go:125] createHost starting for "m03" (driver="docker")
I1011 13:50:01.989505  311659 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I1011 13:50:01.989636  311659 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1011 13:50:01.989658  311659 client.go:168] LocalClient.Create starting
I1011 13:50:01.989759  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/ca.pem
I1011 13:50:01.989809  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:50:01.989828  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:50:01.989901  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/cert.pem
I1011 13:50:01.989923  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:50:01.989944  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:50:01.990256  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:50:02.032455  311659 network_create.go:76] Found existing network {name:minikube subnet:0xc0005fad80 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I1011 13:50:02.032486  311659 kic.go:106] calculated static IP "192.168.49.4" for the "minikube-m03" container
I1011 13:50:02.032561  311659 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1011 13:50:02.073608  311659 cli_runner.go:164] Run: docker volume create minikube-m03 --label name.minikube.sigs.k8s.io=minikube-m03 --label created_by.minikube.sigs.k8s.io=true
I1011 13:50:02.106540  311659 oci.go:103] Successfully created a docker volume minikube-m03
I1011 13:50:02.106609  311659 cli_runner.go:164] Run: docker run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I1011 13:50:02.906408  311659 oci.go:107] Successfully prepared a docker volume minikube-m03
I1011 13:50:02.906433  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:50:02.906449  311659 kic.go:179] Starting extracting preloaded images to volume ...
I1011 13:50:02.906503  311659 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
I1011 13:50:06.247937  311659 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (3.341327038s)
I1011 13:50:06.247995  311659 kic.go:188] duration metric: took 3.341527 seconds to extract preloaded images to volume
W1011 13:50:06.248332  311659 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1011 13:50:06.248600  311659 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1011 13:50:06.580432  311659 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I1011 13:50:07.121793  311659 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Running}}
I1011 13:50:07.147399  311659 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I1011 13:50:07.172768  311659 cli_runner.go:164] Run: docker exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables
I1011 13:50:07.236300  311659 oci.go:144] the created container "minikube-m03" has a running status.
I1011 13:50:07.236618  311659 kic.go:210] Creating ssh key for kic: /home/burak/.minikube/machines/minikube-m03/id_rsa...
I1011 13:50:07.417857  311659 kic_runner.go:191] docker (temp): /home/burak/.minikube/machines/minikube-m03/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1011 13:50:07.549862  311659 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I1011 13:50:07.598651  311659 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1011 13:50:07.598661  311659 kic_runner.go:114] Args: [docker exec --privileged minikube-m03 chown docker:docker /home/docker/.ssh/authorized_keys]
I1011 13:50:07.710709  311659 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I1011 13:50:07.745661  311659 machine.go:88] provisioning docker machine ...
I1011 13:50:07.748140  311659 ubuntu.go:169] provisioning hostname "minikube-m03"
I1011 13:50:07.748208  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:07.785868  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:07.786222  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49187 <nil> <nil>}
I1011 13:50:07.786235  311659 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I1011 13:50:07.937063  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube-m03

I1011 13:50:07.937117  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:07.962876  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:07.963011  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49187 <nil> <nil>}
I1011 13:50:07.963028  311659 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I1011 13:50:08.102843  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1011 13:50:08.102876  311659 ubuntu.go:175] set auth options {CertDir:/home/burak/.minikube CaCertPath:/home/burak/.minikube/certs/ca.pem CaPrivateKeyPath:/home/burak/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/burak/.minikube/machines/server.pem ServerKeyPath:/home/burak/.minikube/machines/server-key.pem ClientKeyPath:/home/burak/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/burak/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/burak/.minikube}
I1011 13:50:08.102914  311659 ubuntu.go:177] setting up certificates
I1011 13:50:08.102933  311659 provision.go:83] configureAuth start
I1011 13:50:08.103061  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1011 13:50:08.193783  311659 provision.go:138] copyHostCerts
I1011 13:50:08.193836  311659 exec_runner.go:144] found /home/burak/.minikube/key.pem, removing ...
I1011 13:50:08.193848  311659 exec_runner.go:207] rm: /home/burak/.minikube/key.pem
I1011 13:50:08.193927  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/key.pem --> /home/burak/.minikube/key.pem (1679 bytes)
I1011 13:50:08.194024  311659 exec_runner.go:144] found /home/burak/.minikube/ca.pem, removing ...
I1011 13:50:08.194029  311659 exec_runner.go:207] rm: /home/burak/.minikube/ca.pem
I1011 13:50:08.194067  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/ca.pem --> /home/burak/.minikube/ca.pem (1074 bytes)
I1011 13:50:08.194139  311659 exec_runner.go:144] found /home/burak/.minikube/cert.pem, removing ...
I1011 13:50:08.194144  311659 exec_runner.go:207] rm: /home/burak/.minikube/cert.pem
I1011 13:50:08.194185  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/cert.pem --> /home/burak/.minikube/cert.pem (1119 bytes)
I1011 13:50:08.194277  311659 provision.go:112] generating server cert: /home/burak/.minikube/machines/server.pem ca-key=/home/burak/.minikube/certs/ca.pem private-key=/home/burak/.minikube/certs/ca-key.pem org=burak.minikube-m03 san=[192.168.49.4 127.0.0.1 localhost 127.0.0.1 minikube minikube-m03]
I1011 13:50:08.420154  311659 provision.go:172] copyRemoteCerts
I1011 13:50:08.420198  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1011 13:50:08.420234  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:08.453595  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49187 SSHKeyPath:/home/burak/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1011 13:50:08.565267  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1011 13:50:08.583931  311659 ssh_runner.go:362] scp /home/burak/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1011 13:50:08.609199  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I1011 13:50:08.637319  311659 provision.go:86] duration metric: configureAuth took 534.374743ms
I1011 13:50:08.637333  311659 ubuntu.go:193] setting minikube options for container-runtime
I1011 13:50:08.637494  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:08.637534  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:08.710449  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:08.710557  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49187 <nil> <nil>}
I1011 13:50:08.710563  311659 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1011 13:50:08.855238  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1011 13:50:08.855250  311659 ubuntu.go:71] root file system type: overlay
I1011 13:50:08.855400  311659 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1011 13:50:08.855448  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:08.875989  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:08.876121  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49187 <nil> <nil>}
I1011 13:50:08.876189  311659 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1011 13:50:09.039055  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1011 13:50:09.039107  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:09.071491  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:09.071641  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49187 <nil> <nil>}
I1011 13:50:09.071663  311659 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1011 13:50:09.831584  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-10-11 10:50:09.029741946 +0000
@@ -1,30 +1,34 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=NO_PROXY=192.168.49.2
+Environment=NO_PROXY=192.168.49.2,192.168.49.3
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +36,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1011 13:50:09.831598  311659 machine.go:91] provisioned docker machine in 2.085928383s
I1011 13:50:09.831604  311659 client.go:171] LocalClient.Create took 7.841941846s
I1011 13:50:09.831613  311659 start.go:167] duration metric: libmachine.API.Create for "minikube" took 7.841980749s
I1011 13:50:09.831619  311659 start.go:300] post-start starting for "minikube-m03" (driver="docker")
I1011 13:50:09.831758  311659 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1011 13:50:09.831805  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1011 13:50:09.831838  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:09.861550  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49187 SSHKeyPath:/home/burak/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1011 13:50:09.959589  311659 ssh_runner.go:195] Run: cat /etc/os-release
I1011 13:50:09.962522  311659 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1011 13:50:09.962542  311659 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1011 13:50:09.962554  311659 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1011 13:50:09.962560  311659 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1011 13:50:09.962569  311659 filesync.go:126] Scanning /home/burak/.minikube/addons for local assets ...
I1011 13:50:09.962613  311659 filesync.go:126] Scanning /home/burak/.minikube/files for local assets ...
I1011 13:50:09.962629  311659 start.go:303] post-start completed in 130.874134ms
I1011 13:50:09.962910  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1011 13:50:09.996152  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:50:09.996317  311659 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1011 13:50:09.996340  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:10.023270  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49187 SSHKeyPath:/home/burak/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1011 13:50:10.121499  311659 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1011 13:50:10.133517  311659 start.go:128] duration metric: createHost completed in 8.147181029s
I1011 13:50:10.133544  311659 start.go:83] releasing machines lock for "minikube-m03", held for 8.14731215s
I1011 13:50:10.133711  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1011 13:50:10.238325  311659 out.go:177] üåê  Found network options:
I1011 13:50:10.244754  311659 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
W1011 13:50:10.251185  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:10.251211  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:10.251237  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:10.251249  311659 proxy.go:119] fail to check proxy env: Error ip not in block
I1011 13:50:10.251379  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1011 13:50:10.251396  311659 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1011 13:50:10.251419  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:10.251441  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:10.298182  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49187 SSHKeyPath:/home/burak/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1011 13:50:10.316393  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49187 SSHKeyPath:/home/burak/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1011 13:50:10.414572  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (234 bytes)
I1011 13:50:10.455870  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:50:10.651962  311659 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1011 13:50:10.720866  311659 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1011 13:50:10.729502  311659 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1011 13:50:10.729545  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1011 13:50:10.743742  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1011 13:50:10.756844  311659 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1011 13:50:10.829048  311659 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1011 13:50:10.990679  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:50:11.061643  311659 ssh_runner.go:195] Run: sudo systemctl restart docker
I1011 13:50:11.381223  311659 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1011 13:50:11.461995  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:50:11.601658  311659 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1011 13:50:11.608425  311659 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1011 13:50:11.608459  311659 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1011 13:50:11.610603  311659 start.go:471] Will wait 60s for crictl version
I1011 13:50:11.610633  311659 ssh_runner.go:195] Run: sudo crictl version
I1011 13:50:11.728761  311659 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1011 13:50:11.728871  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:50:11.805344  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:50:11.841929  311659 out.go:204] üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
I1011 13:50:11.853491  311659 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I1011 13:50:11.860016  311659 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
I1011 13:50:11.866145  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:50:11.890979  311659 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1011 13:50:11.893403  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:50:11.901805  311659 certs.go:54] Setting up /home/burak/.minikube/profiles/minikube for IP: 192.168.49.4
I1011 13:50:11.901896  311659 certs.go:182] skipping minikubeCA CA generation: /home/burak/.minikube/ca.key
I1011 13:50:11.901932  311659 certs.go:182] skipping proxyClientCA CA generation: /home/burak/.minikube/proxy-client-ca.key
I1011 13:50:11.901996  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca-key.pem (1679 bytes)
I1011 13:50:11.902019  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca.pem (1074 bytes)
I1011 13:50:11.902042  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/cert.pem (1119 bytes)
I1011 13:50:11.902064  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/key.pem (1679 bytes)
I1011 13:50:11.903326  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1011 13:50:11.920757  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1011 13:50:11.938210  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1011 13:50:11.953598  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1011 13:50:11.975045  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1011 13:50:11.988810  311659 ssh_runner.go:195] Run: openssl version
I1011 13:50:11.993577  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1011 13:50:11.999971  311659 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1011 13:50:12.002465  311659 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Oct  5 02:47 /usr/share/ca-certificates/minikubeCA.pem
I1011 13:50:12.002501  311659 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1011 13:50:12.006854  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1011 13:50:12.012814  311659 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1011 13:50:12.162078  311659 cni.go:95] Creating CNI manager for ""
I1011 13:50:12.162085  311659 cni.go:156] 3 nodes found, recommending kindnet
I1011 13:50:12.162093  311659 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1011 13:50:12.162106  311659 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.4 APIServerPort:8443 KubernetesVersion:v1.25.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m03 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.4 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:true}
I1011 13:50:12.162393  311659 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.4
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube-m03"
  kubeletExtraArgs:
    node-ip: 192.168.49.4
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
resolvConf: /etc/kubelet-resolv.conf
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1011 13:50:12.162435  311659 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube-m03 --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.4 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1011 13:50:12.162469  311659 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.0
I1011 13:50:12.168343  311659 binaries.go:44] Found k8s binaries, skipping transfer
I1011 13:50:12.168382  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1011 13:50:12.174326  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (474 bytes)
I1011 13:50:12.185974  311659 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1011 13:50:12.196855  311659 ssh_runner.go:195] Run: sudo cp /etc/resolv.conf /etc/kubelet-resolv.conf
I1011 13:50:12.202539  311659 ssh_runner.go:195] Run: sudo sed -i -e "s/^search .$//" /etc/kubelet-resolv.conf
I1011 13:50:12.208732  311659 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1011 13:50:12.211207  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:50:12.218492  311659 host.go:66] Checking if "minikube" exists ...
I1011 13:50:12.218650  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:12.218654  311659 start.go:285] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1011 13:50:12.218707  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I1011 13:50:12.218698  311659 cache.go:107] acquiring lock: {Name:mk9d245b52f2a15608b7182fe894299c8834db79 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:50:12.218736  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:50:12.218781  311659 cache.go:115] /home/burak/.minikube/cache/images/amd64/alpine_latest exists
I1011 13:50:12.218794  311659 cache.go:96] cache image "alpine:latest" -> "/home/burak/.minikube/cache/images/amd64/alpine_latest" took 102.209¬µs
I1011 13:50:12.218800  311659 cache.go:80] save to tar file alpine:latest -> /home/burak/.minikube/cache/images/amd64/alpine_latest succeeded
I1011 13:50:12.218805  311659 cache.go:87] Successfully saved all images to host disk.
I1011 13:50:12.218906  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:12.219097  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:50:12.245787  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:50:12.251959  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:50:12.251987  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:50:12.276680  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:50:12.418637  311659 start.go:306] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:50:12.418662  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token u6014d.c10v74y3u4ct73de --discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03"
I1011 13:50:12.418751  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
alpine:latest
kindest/kindnetd:v20220726-ed811e41
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:50:12.418757  311659 cache_images.go:84] Images are preloaded, skipping loading
I1011 13:50:12.418984  311659 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1011 13:50:12.438933  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:50:12.438954  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:50:12.459737  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:50:18.348153  311659 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token u6014d.c10v74y3u4ct73de --discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03": (5.929447451s)
I1011 13:50:18.348205  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1011 13:50:18.348673  311659 ssh_runner.go:235] Completed: docker images --format {{.Repository}}:{{.Tag}}: (5.909702486s)
I1011 13:50:18.348724  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
alpine:latest
kindest/kindnetd:v20220726-ed811e41
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:50:18.348747  311659 cache_images.go:84] Images are preloaded, skipping loading
I1011 13:50:18.349643  311659 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I1011 13:50:18.432623  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:50:18.432653  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:18.480570  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49187 SSHKeyPath:/home/burak/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1011 13:50:18.692696  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:50:18.692710  311659 docker.go:617] alpine:latest wasn't preloaded
I1011 13:50:18.692715  311659 cache_images.go:88] LoadImages start: [alpine:latest]
I1011 13:50:18.692844  311659 start.go:287] JoinCluster complete in 6.474188555s
I1011 13:50:18.692851  311659 cni.go:95] Creating CNI manager for ""
I1011 13:50:18.692855  311659 cni.go:156] 3 nodes found, recommending kindnet
I1011 13:50:18.692891  311659 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1011 13:50:18.696826  311659 image.go:134] retrieving image: alpine:latest
I1011 13:50:18.696838  311659 image.go:140] checking repository: index.docker.io/library/alpine
I1011 13:50:18.706633  311659 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.25.0/kubectl ...
I1011 13:50:18.706643  311659 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I1011 13:50:18.729395  311659 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1011 13:50:18.896569  311659 start.go:211] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:50:18.903017  311659 out.go:177] üîé  Verifying Kubernetes components...
I1011 13:50:18.908853  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1011 13:50:18.919840  311659 kubeadm.go:573] duration metric: took 23.240549ms to wait for : map[apiserver:true system_pods:true] ...
I1011 13:50:18.919859  311659 node_conditions.go:102] verifying NodePressure condition ...
I1011 13:50:18.924308  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:50:18.924320  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:50:18.924330  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:50:18.924335  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:50:18.924339  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:50:18.924343  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:50:18.924347  311659 node_conditions.go:105] duration metric: took 4.484323ms to run NodePressure ...
I1011 13:50:18.924356  311659 start.go:216] waiting for startup goroutines ...
I1011 13:50:20.039947  311659 image.go:144] canonical name: docker.io/library/alpine:latest
I1011 13:50:20.040666  311659 image.go:177] daemon lookup for alpine:latest: Error: No such image: alpine:latest
I1011 13:50:21.039628  311659 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} alpine:latest
I1011 13:50:21.109243  311659 cache_images.go:116] "alpine:latest" needs transfer: "alpine:latest" does not exist at hash "9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5" in container runtime
I1011 13:50:21.109278  311659 docker.go:292] Removing image: alpine:latest
I1011 13:50:21.109318  311659 ssh_runner.go:195] Run: docker rmi alpine:latest
I1011 13:50:21.141921  311659 cache_images.go:286] Loading image from: /home/burak/.minikube/cache/images/amd64/alpine_latest
I1011 13:50:21.141977  311659 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest
I1011 13:50:21.144092  311659 ssh_runner.go:352] existence check for /var/lib/minikube/images/alpine_latest: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/alpine_latest': No such file or directory
I1011 13:50:21.144107  311659 ssh_runner.go:362] scp /home/burak/.minikube/cache/images/amd64/alpine_latest --> /var/lib/minikube/images/alpine_latest (2810880 bytes)
I1011 13:50:21.166804  311659 docker.go:259] Loading image: /var/lib/minikube/images/alpine_latest
I1011 13:50:21.166817  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/alpine_latest | docker load"
I1011 13:50:21.345707  311659 cache_images.go:315] Transferred and loaded /home/burak/.minikube/cache/images/amd64/alpine_latest from cache
I1011 13:50:21.345725  311659 cache_images.go:123] Successfully loaded all cached images
I1011 13:50:21.345728  311659 cache_images.go:92] LoadImages completed in 2.653007792s
I1011 13:50:21.345758  311659 cache_images.go:262] succeeded pushing to: minikube minikube-m02 minikube-m03
I1011 13:50:21.345762  311659 cache_images.go:263] failed pushing to: 
I1011 13:50:21.351739  311659 out.go:177] 
I1011 13:50:21.357985  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:21.358099  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:50:21.364856  311659 out.go:177] üëç  Starting worker node minikube-m04 in cluster minikube
I1011 13:50:21.371043  311659 cache.go:120] Beginning downloading kic base image for docker with docker
I1011 13:50:21.376941  311659 out.go:177] üöú  Pulling base image ...
I1011 13:50:21.388966  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:50:21.388986  311659 cache.go:57] Caching tarball of preloaded images
I1011 13:50:21.389035  311659 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I1011 13:50:21.389107  311659 preload.go:174] Found /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1011 13:50:21.389128  311659 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.0 on docker
I1011 13:50:21.389214  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:50:21.417978  311659 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I1011 13:50:21.417989  311659 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I1011 13:50:21.417998  311659 cache.go:208] Successfully downloaded all kic artifacts
I1011 13:50:21.418017  311659 start.go:364] acquiring machines lock for minikube-m04: {Name:mk6ce616df855e58434b38401180427c27575f48 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:50:21.418091  311659 start.go:368] acquired machines lock for "minikube-m04" in 62.555¬µs
I1011 13:50:21.418102  311659 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP: Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name:m04 IP: Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:50:21.418154  311659 start.go:125] createHost starting for "m04" (driver="docker")
I1011 13:50:21.424416  311659 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I1011 13:50:21.424513  311659 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1011 13:50:21.424533  311659 client.go:168] LocalClient.Create starting
I1011 13:50:21.424587  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/ca.pem
I1011 13:50:21.424613  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:50:21.424629  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:50:21.425002  311659 main.go:134] libmachine: Reading certificate data from /home/burak/.minikube/certs/cert.pem
I1011 13:50:21.425021  311659 main.go:134] libmachine: Decoding PEM data...
I1011 13:50:21.425032  311659 main.go:134] libmachine: Parsing certificate...
I1011 13:50:21.425259  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:50:21.446489  311659 network_create.go:76] Found existing network {name:minikube subnet:0xc0016aff80 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I1011 13:50:21.446523  311659 kic.go:106] calculated static IP "192.168.49.5" for the "minikube-m04" container
I1011 13:50:21.446632  311659 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1011 13:50:21.470515  311659 cli_runner.go:164] Run: docker volume create minikube-m04 --label name.minikube.sigs.k8s.io=minikube-m04 --label created_by.minikube.sigs.k8s.io=true
I1011 13:50:21.501477  311659 oci.go:103] Successfully created a docker volume minikube-m04
I1011 13:50:21.501519  311659 cli_runner.go:164] Run: docker run --rm --name minikube-m04-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m04 --entrypoint /usr/bin/test -v minikube-m04:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I1011 13:50:22.434015  311659 oci.go:107] Successfully prepared a docker volume minikube-m04
I1011 13:50:22.434050  311659 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I1011 13:50:22.434067  311659 kic.go:179] Starting extracting preloaded images to volume ...
I1011 13:50:22.434118  311659 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m04:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
I1011 13:50:27.769058  311659 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/burak/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m04:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (5.3348273s)
I1011 13:50:27.769118  311659 kic.go:188] duration metric: took 5.335025 seconds to extract preloaded images to volume
W1011 13:50:27.769707  311659 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1011 13:50:27.770157  311659 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1011 13:50:27.991120  311659 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m04 --name minikube-m04 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m04 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m04 --network minikube --ip 192.168.49.5 --volume minikube-m04:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I1011 13:50:28.603710  311659 cli_runner.go:164] Run: docker container inspect minikube-m04 --format={{.State.Running}}
I1011 13:50:28.670529  311659 cli_runner.go:164] Run: docker container inspect minikube-m04 --format={{.State.Status}}
I1011 13:50:28.700861  311659 cli_runner.go:164] Run: docker exec minikube-m04 stat /var/lib/dpkg/alternatives/iptables
I1011 13:50:28.811128  311659 oci.go:144] the created container "minikube-m04" has a running status.
I1011 13:50:28.811144  311659 kic.go:210] Creating ssh key for kic: /home/burak/.minikube/machines/minikube-m04/id_rsa...
I1011 13:50:28.926479  311659 kic_runner.go:191] docker (temp): /home/burak/.minikube/machines/minikube-m04/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1011 13:50:29.052393  311659 cli_runner.go:164] Run: docker container inspect minikube-m04 --format={{.State.Status}}
I1011 13:50:29.087960  311659 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1011 13:50:29.087974  311659 kic_runner.go:114] Args: [docker exec --privileged minikube-m04 chown docker:docker /home/docker/.ssh/authorized_keys]
I1011 13:50:29.203404  311659 cli_runner.go:164] Run: docker container inspect minikube-m04 --format={{.State.Status}}
I1011 13:50:29.243989  311659 machine.go:88] provisioning docker machine ...
I1011 13:50:29.244017  311659 ubuntu.go:169] provisioning hostname "minikube-m04"
I1011 13:50:29.244066  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:29.280552  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:29.280707  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49192 <nil> <nil>}
I1011 13:50:29.280718  311659 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube-m04 && echo "minikube-m04" | sudo tee /etc/hostname
I1011 13:50:29.424149  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube-m04

I1011 13:50:29.424193  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:29.446037  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:29.446170  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49192 <nil> <nil>}
I1011 13:50:29.446181  311659 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m04' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m04/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m04' | sudo tee -a /etc/hosts; 
			fi
		fi
I1011 13:50:29.606940  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1011 13:50:29.606979  311659 ubuntu.go:175] set auth options {CertDir:/home/burak/.minikube CaCertPath:/home/burak/.minikube/certs/ca.pem CaPrivateKeyPath:/home/burak/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/burak/.minikube/machines/server.pem ServerKeyPath:/home/burak/.minikube/machines/server-key.pem ClientKeyPath:/home/burak/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/burak/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/burak/.minikube}
I1011 13:50:29.607006  311659 ubuntu.go:177] setting up certificates
I1011 13:50:29.607020  311659 provision.go:83] configureAuth start
I1011 13:50:29.607156  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m04
I1011 13:50:29.684503  311659 provision.go:138] copyHostCerts
I1011 13:50:29.684550  311659 exec_runner.go:144] found /home/burak/.minikube/ca.pem, removing ...
I1011 13:50:29.684569  311659 exec_runner.go:207] rm: /home/burak/.minikube/ca.pem
I1011 13:50:29.684640  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/ca.pem --> /home/burak/.minikube/ca.pem (1074 bytes)
I1011 13:50:29.684718  311659 exec_runner.go:144] found /home/burak/.minikube/cert.pem, removing ...
I1011 13:50:29.684722  311659 exec_runner.go:207] rm: /home/burak/.minikube/cert.pem
I1011 13:50:29.684751  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/cert.pem --> /home/burak/.minikube/cert.pem (1119 bytes)
I1011 13:50:29.684799  311659 exec_runner.go:144] found /home/burak/.minikube/key.pem, removing ...
I1011 13:50:29.684803  311659 exec_runner.go:207] rm: /home/burak/.minikube/key.pem
I1011 13:50:29.684835  311659 exec_runner.go:151] cp: /home/burak/.minikube/certs/key.pem --> /home/burak/.minikube/key.pem (1679 bytes)
I1011 13:50:29.684883  311659 provision.go:112] generating server cert: /home/burak/.minikube/machines/server.pem ca-key=/home/burak/.minikube/certs/ca.pem private-key=/home/burak/.minikube/certs/ca-key.pem org=burak.minikube-m04 san=[192.168.49.5 127.0.0.1 localhost 127.0.0.1 minikube minikube-m04]
I1011 13:50:29.868253  311659 provision.go:172] copyRemoteCerts
I1011 13:50:29.868318  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1011 13:50:29.868341  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:29.899452  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49192 SSHKeyPath:/home/burak/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1011 13:50:30.009500  311659 ssh_runner.go:362] scp /home/burak/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1011 13:50:30.063782  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I1011 13:50:30.124194  311659 ssh_runner.go:362] scp /home/burak/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1011 13:50:30.158717  311659 provision.go:86] duration metric: configureAuth took 551.684443ms
I1011 13:50:30.158739  311659 ubuntu.go:193] setting minikube options for container-runtime
I1011 13:50:30.158998  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:30.159055  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:30.203241  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:30.203460  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49192 <nil> <nil>}
I1011 13:50:30.203475  311659 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1011 13:50:30.348921  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1011 13:50:30.348930  311659 ubuntu.go:71] root file system type: overlay
I1011 13:50:30.349045  311659 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1011 13:50:30.349080  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:30.367703  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:30.367800  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49192 <nil> <nil>}
I1011 13:50:30.367848  311659 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"
Environment="NO_PROXY=192.168.49.2,192.168.49.3,192.168.49.4"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1011 13:50:30.536366  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3
Environment=NO_PROXY=192.168.49.2,192.168.49.3,192.168.49.4


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1011 13:50:30.536527  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:30.588003  311659 main.go:134] libmachine: Using SSH client type: native
I1011 13:50:30.588182  311659 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ec520] 0x7ef6a0 <nil>  [] 0s} 127.0.0.1 49192 <nil> <nil>}
I1011 13:50:30.588205  311659 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1011 13:50:31.712673  311659 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-10-11 10:50:30.529530472 +0000
@@ -1,30 +1,35 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=NO_PROXY=192.168.49.2
+Environment=NO_PROXY=192.168.49.2,192.168.49.3
+Environment=NO_PROXY=192.168.49.2,192.168.49.3,192.168.49.4
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +37,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1011 13:50:31.712717  311659 machine.go:91] provisioned docker machine in 2.468703445s
I1011 13:50:31.712739  311659 client.go:171] LocalClient.Create took 10.28819646s
I1011 13:50:31.712771  311659 start.go:167] duration metric: libmachine.API.Create for "minikube" took 10.288246012s
I1011 13:50:31.712790  311659 start.go:300] post-start starting for "minikube-m04" (driver="docker")
I1011 13:50:31.712811  311659 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1011 13:50:31.712958  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1011 13:50:31.713072  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:31.817723  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49192 SSHKeyPath:/home/burak/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1011 13:50:31.933324  311659 ssh_runner.go:195] Run: cat /etc/os-release
I1011 13:50:31.942753  311659 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1011 13:50:31.942807  311659 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1011 13:50:31.942848  311659 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1011 13:50:31.942866  311659 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1011 13:50:31.942890  311659 filesync.go:126] Scanning /home/burak/.minikube/addons for local assets ...
I1011 13:50:31.943031  311659 filesync.go:126] Scanning /home/burak/.minikube/files for local assets ...
I1011 13:50:31.943090  311659 start.go:303] post-start completed in 230.285027ms
I1011 13:50:31.943825  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m04
I1011 13:50:32.022519  311659 profile.go:148] Saving config to /home/burak/.minikube/profiles/minikube/config.json ...
I1011 13:50:32.023046  311659 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1011 13:50:32.023121  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:32.096168  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49192 SSHKeyPath:/home/burak/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1011 13:50:32.222022  311659 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1011 13:50:32.245953  311659 start.go:128] duration metric: createHost completed in 10.827778914s
I1011 13:50:32.245988  311659 start.go:83] releasing machines lock for "minikube-m04", held for 10.827881948s
I1011 13:50:32.246302  311659 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m04
I1011 13:50:32.345261  311659 out.go:177] üåê  Found network options:
I1011 13:50:32.351749  311659 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3,192.168.49.4
W1011 13:50:32.357883  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:32.357916  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:32.357936  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:32.357970  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:32.357983  311659 proxy.go:119] fail to check proxy env: Error ip not in block
W1011 13:50:32.357995  311659 proxy.go:119] fail to check proxy env: Error ip not in block
I1011 13:50:32.358143  311659 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1011 13:50:32.358147  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1011 13:50:32.358211  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:32.358236  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:32.404900  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49192 SSHKeyPath:/home/burak/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1011 13:50:32.404931  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49192 SSHKeyPath:/home/burak/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1011 13:50:32.510988  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (234 bytes)
I1011 13:50:32.551369  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:50:32.759741  311659 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1011 13:50:32.816306  311659 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1011 13:50:32.824103  311659 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1011 13:50:32.824133  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1011 13:50:32.831000  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1011 13:50:32.840006  311659 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1011 13:50:32.912077  311659 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1011 13:50:32.986612  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:50:33.123239  311659 ssh_runner.go:195] Run: sudo systemctl restart docker
I1011 13:50:33.446373  311659 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1011 13:50:33.580866  311659 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1011 13:50:33.646798  311659 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1011 13:50:33.653583  311659 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1011 13:50:33.653618  311659 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1011 13:50:33.656650  311659 start.go:471] Will wait 60s for crictl version
I1011 13:50:33.656694  311659 ssh_runner.go:195] Run: sudo crictl version
I1011 13:50:33.683592  311659 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1011 13:50:33.683636  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:50:33.751097  311659 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1011 13:50:33.783882  311659 out.go:204] üê≥  Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
I1011 13:50:33.789871  311659 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I1011 13:50:33.798386  311659 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
I1011 13:50:33.808783  311659 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3,192.168.49.4
I1011 13:50:33.816620  311659 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1011 13:50:33.903012  311659 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1011 13:50:33.907160  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:50:33.918991  311659 certs.go:54] Setting up /home/burak/.minikube/profiles/minikube for IP: 192.168.49.5
I1011 13:50:33.919067  311659 certs.go:182] skipping minikubeCA CA generation: /home/burak/.minikube/ca.key
I1011 13:50:33.919101  311659 certs.go:182] skipping proxyClientCA CA generation: /home/burak/.minikube/proxy-client-ca.key
I1011 13:50:33.919159  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca-key.pem (1679 bytes)
I1011 13:50:33.919179  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/ca.pem (1074 bytes)
I1011 13:50:33.919196  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/cert.pem (1119 bytes)
I1011 13:50:33.919217  311659 certs.go:388] found cert: /home/burak/.minikube/certs/home/burak/.minikube/certs/key.pem (1679 bytes)
I1011 13:50:33.919521  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1011 13:50:33.936571  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1011 13:50:33.953505  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1011 13:50:33.966554  311659 ssh_runner.go:362] scp /home/burak/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1011 13:50:33.979257  311659 ssh_runner.go:362] scp /home/burak/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1011 13:50:33.992716  311659 ssh_runner.go:195] Run: openssl version
I1011 13:50:33.997175  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1011 13:50:34.002715  311659 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1011 13:50:34.005367  311659 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Oct  5 02:47 /usr/share/ca-certificates/minikubeCA.pem
I1011 13:50:34.005398  311659 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1011 13:50:34.009043  311659 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1011 13:50:34.014498  311659 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1011 13:50:34.096668  311659 cni.go:95] Creating CNI manager for ""
I1011 13:50:34.096674  311659 cni.go:156] 4 nodes found, recommending kindnet
I1011 13:50:34.096682  311659 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1011 13:50:34.096692  311659 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.5 APIServerPort:8443 KubernetesVersion:v1.25.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m04 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.5 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:true}
I1011 13:50:34.096761  311659 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.5
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube-m04"
  kubeletExtraArgs:
    node-ip: 192.168.49.5
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
resolvConf: /etc/kubelet-resolv.conf
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1011 13:50:34.096798  311659 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube-m04 --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.5 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1011 13:50:34.096829  311659 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.0
I1011 13:50:34.102000  311659 binaries.go:44] Found k8s binaries, skipping transfer
I1011 13:50:34.102032  311659 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1011 13:50:34.107497  311659 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (474 bytes)
I1011 13:50:34.117305  311659 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1011 13:50:34.126877  311659 ssh_runner.go:195] Run: sudo cp /etc/resolv.conf /etc/kubelet-resolv.conf
I1011 13:50:34.131963  311659 ssh_runner.go:195] Run: sudo sed -i -e "s/^search .$//" /etc/kubelet-resolv.conf
I1011 13:50:34.137242  311659 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1011 13:50:34.139316  311659 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1011 13:50:34.145979  311659 host.go:66] Checking if "minikube" exists ...
I1011 13:50:34.146181  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:34.146163  311659 start.go:285] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/burak:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1011 13:50:34.146224  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I1011 13:50:34.146225  311659 cache.go:107] acquiring lock: {Name:mk9d245b52f2a15608b7182fe894299c8834db79 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1011 13:50:34.146258  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:50:34.146303  311659 cache.go:115] /home/burak/.minikube/cache/images/amd64/alpine_latest exists
I1011 13:50:34.146315  311659 cache.go:96] cache image "alpine:latest" -> "/home/burak/.minikube/cache/images/amd64/alpine_latest" took 94.92¬µs
I1011 13:50:34.146321  311659 cache.go:80] save to tar file alpine:latest -> /home/burak/.minikube/cache/images/amd64/alpine_latest succeeded
I1011 13:50:34.146329  311659 cache.go:87] Successfully saved all images to host disk.
I1011 13:50:34.146437  311659 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1011 13:50:34.146638  311659 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1011 13:50:34.173351  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:50:34.175977  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:50:34.175999  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1011 13:50:34.197246  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49177 SSHKeyPath:/home/burak/.minikube/machines/minikube/id_rsa Username:docker}
I1011 13:50:34.417410  311659 start.go:306] trying to join worker node "m04" to cluster: &{Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:50:34.417438  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token c4c2g0.9kz5nkq4buz45t8m --discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m04"
I1011 13:50:34.417490  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
alpine:latest
kindest/kindnetd:v20220726-ed811e41
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:50:34.417501  311659 cache_images.go:84] Images are preloaded, skipping loading
I1011 13:50:34.417927  311659 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1011 13:50:34.450153  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:50:34.450191  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1011 13:50:34.488459  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49182 SSHKeyPath:/home/burak/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1011 13:50:40.366025  311659 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token c4c2g0.9kz5nkq4buz45t8m --discovery-token-ca-cert-hash sha256:18cdbbd669870c16909b1e1bb31b382e2d404a1465e9964bd336723f8b96bb27 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m04": (5.94857256s)
I1011 13:50:40.366039  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1011 13:50:40.366058  311659 ssh_runner.go:235] Completed: docker images --format {{.Repository}}:{{.Tag}}: (5.915892477s)
I1011 13:50:40.366076  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
alpine:latest
kindest/kindnetd:v20220726-ed811e41
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:50:40.366083  311659 cache_images.go:84] Images are preloaded, skipping loading
I1011 13:50:40.366305  311659 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I1011 13:50:40.403686  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:50:40.403709  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1011 13:50:40.439406  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49187 SSHKeyPath:/home/burak/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1011 13:50:40.652626  311659 start.go:287] JoinCluster complete in 6.506459152s
I1011 13:50:40.652638  311659 cni.go:95] Creating CNI manager for ""
I1011 13:50:40.652643  311659 cni.go:156] 4 nodes found, recommending kindnet
I1011 13:50:40.652683  311659 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1011 13:50:40.652722  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
alpine:latest
kindest/kindnetd:v20220726-ed811e41
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:50:40.652732  311659 cache_images.go:84] Images are preloaded, skipping loading
I1011 13:50:40.653042  311659 cli_runner.go:164] Run: docker container inspect minikube-m04 --format={{.State.Status}}
I1011 13:50:40.656770  311659 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.25.0/kubectl ...
I1011 13:50:40.656781  311659 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I1011 13:50:40.672795  311659 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1011 13:50:40.685046  311659 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1011 13:50:40.685071  311659 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1011 13:50:40.711633  311659 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49192 SSHKeyPath:/home/burak/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1011 13:50:40.874245  311659 start.go:211] Will wait 6m0s for node &{Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1011 13:50:40.886461  311659 out.go:177] üîé  Verifying Kubernetes components...
I1011 13:50:40.874280  311659 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1011 13:50:40.906138  311659 docker.go:617] alpine:latest wasn't preloaded
I1011 13:50:40.906185  311659 cache_images.go:88] LoadImages start: [alpine:latest]
I1011 13:50:40.906398  311659 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1011 13:50:40.911755  311659 image.go:134] retrieving image: alpine:latest
I1011 13:50:40.911791  311659 image.go:140] checking repository: index.docker.io/library/alpine
I1011 13:50:40.967739  311659 kubeadm.go:573] duration metric: took 93.444752ms to wait for : map[apiserver:true system_pods:true] ...
I1011 13:50:40.967817  311659 node_conditions.go:102] verifying NodePressure condition ...
I1011 13:50:40.979488  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:50:40.979518  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:50:40.979539  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:50:40.979549  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:50:40.979560  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:50:40.979570  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:50:40.979580  311659 node_conditions.go:122] node storage ephemeral capacity is 76319516Ki
I1011 13:50:40.979590  311659 node_conditions.go:123] node cpu capacity is 4
I1011 13:50:40.979600  311659 node_conditions.go:105] duration metric: took 11.772261ms to run NodePressure ...
I1011 13:50:40.979623  311659 start.go:216] waiting for startup goroutines ...
I1011 13:50:42.231818  311659 image.go:144] canonical name: docker.io/library/alpine:latest
I1011 13:50:42.233979  311659 image.go:177] daemon lookup for alpine:latest: Error: No such image: alpine:latest
I1011 13:50:43.125027  311659 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} alpine:latest
I1011 13:50:43.180702  311659 cache_images.go:116] "alpine:latest" needs transfer: "alpine:latest" does not exist at hash "9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5" in container runtime
I1011 13:50:43.180730  311659 docker.go:292] Removing image: alpine:latest
I1011 13:50:43.180763  311659 ssh_runner.go:195] Run: docker rmi alpine:latest
I1011 13:50:43.204265  311659 cache_images.go:286] Loading image from: /home/burak/.minikube/cache/images/amd64/alpine_latest
I1011 13:50:43.204316  311659 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest
I1011 13:50:43.206857  311659 ssh_runner.go:352] existence check for /var/lib/minikube/images/alpine_latest: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/images/alpine_latest: Process exited with status 1
stdout:

stderr:
stat: cannot stat '/var/lib/minikube/images/alpine_latest': No such file or directory
I1011 13:50:43.206869  311659 ssh_runner.go:362] scp /home/burak/.minikube/cache/images/amd64/alpine_latest --> /var/lib/minikube/images/alpine_latest (2810880 bytes)
I1011 13:50:43.230363  311659 docker.go:259] Loading image: /var/lib/minikube/images/alpine_latest
I1011 13:50:43.230376  311659 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/alpine_latest | docker load"
I1011 13:50:43.439945  311659 cache_images.go:315] Transferred and loaded /home/burak/.minikube/cache/images/amd64/alpine_latest from cache
I1011 13:50:43.439998  311659 cache_images.go:123] Successfully loaded all cached images
I1011 13:50:43.440012  311659 cache_images.go:92] LoadImages completed in 2.533788003s
I1011 13:50:43.440034  311659 cache_images.go:262] succeeded pushing to: minikube minikube-m02 minikube-m03 minikube-m04
I1011 13:50:43.440040  311659 cache_images.go:263] failed pushing to: 
I1011 13:50:43.532416  311659 start.go:506] kubectl: 1.25.2, cluster: 1.25.0 (minor skew: 0)
I1011 13:50:43.538581  311659 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Tue 2022-10-11 10:49:14 UTC, end at Tue 2022-10-11 10:58:08 UTC. --
Oct 11 10:49:15 minikube systemd[1]: Started Docker Application Container Engine.
Oct 11 10:49:15 minikube dockerd[256]: time="2022-10-11T10:49:15.256143895Z" level=info msg="API listen on /run/docker.sock"
Oct 11 10:49:16 minikube systemd[1]: docker.service: Current command vanished from the unit file, execution of the command list won't be resumed.
Oct 11 10:49:17 minikube systemd[1]: Stopping Docker Application Container Engine...
Oct 11 10:49:17 minikube dockerd[256]: time="2022-10-11T10:49:17.089849193Z" level=info msg="Processing signal 'terminated'"
Oct 11 10:49:17 minikube dockerd[256]: time="2022-10-11T10:49:17.091140546Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Oct 11 10:49:17 minikube dockerd[256]: time="2022-10-11T10:49:17.092023761Z" level=info msg="Daemon shutdown complete"
Oct 11 10:49:17 minikube systemd[1]: docker.service: Succeeded.
Oct 11 10:49:17 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 11 10:49:17 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.146271239Z" level=info msg="Starting up"
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.149033742Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.149058600Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.149079624Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.149090999Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.149966674Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.149985891Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.150006953Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.150020060Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.157289956Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.162899543Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.162916973Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.162923111Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.163028839Z" level=info msg="Loading containers: start."
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.220143428Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.338769121Z" level=info msg="Loading containers: done."
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.354314835Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.354433421Z" level=info msg="Daemon has completed initialization"
Oct 11 10:49:17 minikube systemd[1]: Started Docker Application Container Engine.
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.434728640Z" level=info msg="API listen on [::]:2376"
Oct 11 10:49:17 minikube dockerd[492]: time="2022-10-11T10:49:17.455480545Z" level=info msg="API listen on /var/run/docker.sock"
Oct 11 10:49:18 minikube systemd[1]: Stopping Docker Application Container Engine...
Oct 11 10:49:18 minikube dockerd[492]: time="2022-10-11T10:49:18.846481085Z" level=info msg="Processing signal 'terminated'"
Oct 11 10:49:18 minikube dockerd[492]: time="2022-10-11T10:49:18.847917157Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Oct 11 10:49:18 minikube dockerd[492]: time="2022-10-11T10:49:18.849084051Z" level=info msg="Daemon shutdown complete"
Oct 11 10:49:18 minikube dockerd[492]: time="2022-10-11T10:49:18.849129819Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Oct 11 10:49:18 minikube systemd[1]: docker.service: Succeeded.
Oct 11 10:49:18 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 11 10:49:18 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.895033273Z" level=info msg="Starting up"
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.897040960Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.897066555Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.897087584Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.897098380Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.897999938Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.898019412Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.898037347Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.898051700Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.905556927Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.912130772Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.912159881Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.912166614Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.912323694Z" level=info msg="Loading containers: start."
Oct 11 10:49:18 minikube dockerd[750]: time="2022-10-11T10:49:18.993755449Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 11 10:49:19 minikube dockerd[750]: time="2022-10-11T10:49:19.020402319Z" level=info msg="Loading containers: done."
Oct 11 10:49:19 minikube dockerd[750]: time="2022-10-11T10:49:19.031082714Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Oct 11 10:49:19 minikube dockerd[750]: time="2022-10-11T10:49:19.031125944Z" level=info msg="Daemon has completed initialization"
Oct 11 10:49:19 minikube systemd[1]: Started Docker Application Container Engine.
Oct 11 10:49:19 minikube dockerd[750]: time="2022-10-11T10:49:19.052847435Z" level=info msg="API listen on [::]:2376"
Oct 11 10:49:19 minikube dockerd[750]: time="2022-10-11T10:49:19.055848754Z" level=info msg="API listen on /var/run/docker.sock"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID
9fd75790ee214       5185b96f0becf                                                                              7 minutes ago       Running             coredns                   0                   f73f1ad4bda87
e842a88d6ecaf       6e38f40d628db                                                                              7 minutes ago       Running             storage-provisioner       0                   9f23608ad12d7
7f1d7112a849a       kindest/kindnetd@sha256:e2d4d675dcf28a90102ad5219b75c5a0ee096c4321247dfae31dd1467611a9fb   8 minutes ago       Running             kindnet-cni               0                   0fe3bb2bd5358
390ec4a193649       58a9a0c6d96f2                                                                              8 minutes ago       Running             kube-proxy                0                   53f40d48f3ca5
cae214108a059       1a54c86c03a67                                                                              8 minutes ago       Running             kube-controller-manager   0                   23ddf39649c84
136e7ee1f0912       4d2edfd10d3e3                                                                              8 minutes ago       Running             kube-apiserver            0                   8d801cbbd285d
1dd1073146d3d       a8a176a5d5d69                                                                              8 minutes ago       Running             etcd                      0                   dab2919bff3e4
18065028f1969       bef2cf3115095                                                                              8 minutes ago       Running             kube-scheduler            0                   4499105d3600a

* 
* ==> coredns [9fd75790ee21] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_10_11T13_49_39_0700
                    minikube.k8s.io/version=v1.27.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 11 Oct 2022 10:49:34 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 11 Oct 2022 10:58:07 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 11 Oct 2022 10:55:15 +0000   Tue, 11 Oct 2022 10:49:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 11 Oct 2022 10:55:15 +0000   Tue, 11 Oct 2022 10:49:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 11 Oct 2022 10:55:15 +0000   Tue, 11 Oct 2022 10:49:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 11 Oct 2022 10:55:15 +0000   Tue, 11 Oct 2022 10:50:08 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
System Info:
  Machine ID:                 40026c506a9948afaae3b0fda0a61c83
  System UUID:                e7bc4a00-99b1-4bcb-bdd0-34d5487d1cc6
  Boot ID:                    8c77d368-3a4d-4b7b-9b63-5d23ba435cb8
  Kernel Version:             5.15.0-50-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.25.0
  Kube-Proxy Version:         v1.25.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-565d847f94-9xrmh            100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     8m18s
  kube-system                 etcd-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         8m30s
  kube-system                 kindnet-lnnrr                       100m (2%!)(MISSING)     100m (2%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      8m19s
  kube-system                 kube-apiserver-minikube             250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m30s
  kube-system                 kube-controller-manager-minikube    200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m30s
  kube-system                 kube-proxy-669pp                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m19s
  kube-system                 kube-scheduler-minikube             100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m31s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m28s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  100m (2%!)(MISSING)
  memory             220Mi (2%!)(MISSING)  220Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 8m18s                  kube-proxy       
  Normal  NodeHasSufficientMemory  8m45s (x5 over 8m45s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m45s (x5 over 8m45s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m45s (x4 over 8m45s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 8m31s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  8m31s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m31s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m31s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  8m30s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           8m19s                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                8m                     kubelet          Node minikube status is now: NodeReady


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 11 Oct 2022 10:49:53 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Tue, 11 Oct 2022 10:58:04 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 11 Oct 2022 10:58:05 +0000   Tue, 11 Oct 2022 10:49:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 11 Oct 2022 10:58:05 +0000   Tue, 11 Oct 2022 10:49:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 11 Oct 2022 10:58:05 +0000   Tue, 11 Oct 2022 10:49:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 11 Oct 2022 10:58:05 +0000   Tue, 11 Oct 2022 10:50:14 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
System Info:
  Machine ID:                 40026c506a9948afaae3b0fda0a61c83
  System UUID:                7179ed40-ae92-489a-9b01-476173f771e4
  Boot ID:                    8c77d368-3a4d-4b7b-9b63-5d23ba435cb8
  Kernel Version:             5.15.0-50-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.25.0
  Kube-Proxy Version:         v1.25.0
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     django-k8s-web-deployment-fb59f8d56-m5szf    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m30s
  default                     nginx-deployment-596f96d776-j5p29            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m32s
  kube-system                 kindnet-zzgv4                                100m (2%!)(MISSING)     100m (2%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      8m15s
  kube-system                 kube-proxy-2xtmw                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m15s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (2%!)(MISSING)  100m (2%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 8m9s                   kube-proxy       
  Normal  Starting                 8m15s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  8m15s (x2 over 8m15s)  kubelet          Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m15s (x2 over 8m15s)  kubelet          Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m15s (x2 over 8m15s)  kubelet          Node minikube-m02 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  8m15s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           8m14s                  node-controller  Node minikube-m02 event: Registered Node minikube-m02 in Controller
  Normal  NodeReady                7m54s                  kubelet          Node minikube-m02 status is now: NodeReady


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 11 Oct 2022 10:50:13 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Tue, 11 Oct 2022 10:58:01 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 11 Oct 2022 10:57:52 +0000   Tue, 11 Oct 2022 10:50:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 11 Oct 2022 10:57:52 +0000   Tue, 11 Oct 2022 10:50:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 11 Oct 2022 10:57:52 +0000   Tue, 11 Oct 2022 10:50:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 11 Oct 2022 10:57:52 +0000   Tue, 11 Oct 2022 10:50:33 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.4
  Hostname:    minikube-m03
Capacity:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
System Info:
  Machine ID:                 40026c506a9948afaae3b0fda0a61c83
  System UUID:                292f450f-0080-41a2-9be8-8f33ef9b0447
  Boot ID:                    8c77d368-3a4d-4b7b-9b63-5d23ba435cb8
  Kernel Version:             5.15.0-50-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.25.0
  Kube-Proxy Version:         v1.25.0
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     django-k8s-web-deployment-fb59f8d56-q8pc9    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m30s
  default                     nginx-deployment-596f96d776-6kbm4            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m32s
  kube-system                 kindnet-7vv9w                                100m (2%!)(MISSING)     100m (2%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      7m55s
  kube-system                 kube-proxy-7srp2                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m55s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (2%!)(MISSING)  100m (2%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 7m49s                  kube-proxy       
  Normal  Starting                 7m55s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  7m55s (x2 over 7m55s)  kubelet          Node minikube-m03 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    7m55s (x2 over 7m55s)  kubelet          Node minikube-m03 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     7m55s (x2 over 7m55s)  kubelet          Node minikube-m03 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  7m55s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           7m54s                  node-controller  Node minikube-m03 event: Registered Node minikube-m03 in Controller
  Normal  NodeReady                7m35s                  kubelet          Node minikube-m03 status is now: NodeReady


Name:               minikube-m04
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m04
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 11 Oct 2022 10:50:35 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m04
  AcquireTime:     <unset>
  RenewTime:       Tue, 11 Oct 2022 10:58:03 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 11 Oct 2022 10:57:45 +0000   Tue, 11 Oct 2022 10:50:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 11 Oct 2022 10:57:45 +0000   Tue, 11 Oct 2022 10:50:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 11 Oct 2022 10:57:45 +0000   Tue, 11 Oct 2022 10:50:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 11 Oct 2022 10:57:45 +0000   Tue, 11 Oct 2022 10:50:56 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.5
  Hostname:    minikube-m04
Capacity:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  76319516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7876168Ki
  pods:               110
System Info:
  Machine ID:                 40026c506a9948afaae3b0fda0a61c83
  System UUID:                ffcaee6f-f198-481b-a9ac-9cdd0e6c17a4
  Boot ID:                    8c77d368-3a4d-4b7b-9b63-5d23ba435cb8
  Kernel Version:             5.15.0-50-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.25.0
  Kube-Proxy Version:         v1.25.0
PodCIDR:                      10.244.3.0/24
PodCIDRs:                     10.244.3.0/24
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     django-k8s-web-deployment-fb59f8d56-699dk    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m30s
  default                     nginx-deployment-596f96d776-bbb25            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m32s
  kube-system                 kindnet-xstdv                                100m (2%!)(MISSING)     100m (2%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      7m33s
  kube-system                 kube-proxy-d8mrk                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m33s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (2%!)(MISSING)  100m (2%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 7m27s                  kube-proxy       
  Normal  Starting                 7m33s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  7m33s (x2 over 7m33s)  kubelet          Node minikube-m04 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    7m33s (x2 over 7m33s)  kubelet          Node minikube-m04 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     7m33s (x2 over 7m33s)  kubelet          Node minikube-m04 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  7m33s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           7m29s                  node-controller  Node minikube-m04 event: Registered Node minikube-m04 in Controller
  Normal  NodeReady                7m12s                  kubelet          Node minikube-m04 status is now: NodeReady

* 
* ==> dmesg <==
* [  +0.000001] No Arguments are initialized for method [_OFF]

[  +0.000002] ACPI Error: Aborting method \_SB.PCI0.RP05.PCRP._OFF due to previous error (AE_NOT_FOUND) (20210730/psparse-529)
[  +0.027491] done.
[  +0.001760] thermal thermal_zone9: failed to read out thermal zone (-61)
[  +3.128630] kauditd_printk_skb: 24 callbacks suppressed
[Oct11 09:30] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=35968 DF PROTO=2 
[Oct11 09:32] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=41396 DF PROTO=2 
[ +25.481639] show_signal_msg: 22 callbacks suppressed
[  +7.629070] Bluetooth: hci0: MSFT filter_enable is already on
[Oct11 09:34] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=49879 DF PROTO=2 
[Oct11 09:36] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=01:00:5e:00:00:01:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=55408 DF PROTO=2 
[Oct11 09:38] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=1667 DF PROTO=2 
[Oct11 09:40] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=13738 DF PROTO=2 
[Oct11 09:42] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=25634 DF PROTO=2 
[Oct11 09:44] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=32858 DF PROTO=2 
[Oct11 09:47] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=33060 DF PROTO=2 
[Oct11 09:49] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=34903 DF PROTO=2 
[Oct11 09:51] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=39799 DF PROTO=2 
[Oct11 09:53] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=49601 DF PROTO=2 
[Oct11 09:55] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=58741 DF PROTO=2 
[Oct11 09:57] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=63518 DF PROTO=2 
[Oct11 09:59] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=8485 DF PROTO=2 
[Oct11 10:01] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=15556 DF PROTO=2 
[Oct11 10:03] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=25541 DF PROTO=2 
[Oct11 10:05] kauditd_printk_skb: 2 callbacks suppressed
[ +30.488426] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=36966 DF PROTO=2 
[Oct11 10:07] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=42524 DF PROTO=2 
[  +8.009468] kauditd_printk_skb: 2 callbacks suppressed
[Oct11 10:08] kauditd_printk_skb: 44 callbacks suppressed
[Oct11 10:09] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=44231 DF PROTO=2 
[Oct11 10:12] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=53945 DF PROTO=2 
[Oct11 10:14] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=64403 DF PROTO=2 
[Oct11 10:16] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=01:00:5e:00:00:01:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=3906 DF PROTO=2 
[Oct11 10:18] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=5506 DF PROTO=2 
[Oct11 10:20] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=16170 DF PROTO=2 
[Oct11 10:22] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=18419 DF PROTO=2 
[Oct11 10:23] kauditd_printk_skb: 8 callbacks suppressed
[ +21.550796] kauditd_printk_skb: 44 callbacks suppressed
[Oct11 10:24] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=01:00:5e:00:00:01:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=19710 DF PROTO=2 
[Oct11 10:26] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=01:00:5e:00:00:01:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=28956 DF PROTO=2 
[Oct11 10:28] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=30481 DF PROTO=2 
[Oct11 10:30] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=42023 DF PROTO=2 
[Oct11 10:32] kauditd_printk_skb: 8 callbacks suppressed
[  +2.842439] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=53381 DF PROTO=2 
[Oct11 10:33] kauditd_printk_skb: 44 callbacks suppressed
[Oct11 10:34] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=60812 DF PROTO=2 
[Oct11 10:35] kauditd_printk_skb: 8 callbacks suppressed
[Oct11 10:37] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=4534 DF PROTO=2 
[Oct11 10:38] kauditd_printk_skb: 2 callbacks suppressed
[  +5.562778] kauditd_printk_skb: 44 callbacks suppressed
[  +7.428711] kauditd_printk_skb: 8 callbacks suppressed
[Oct11 10:39] kauditd_printk_skb: 94 callbacks suppressed
[ +17.653897] kauditd_printk_skb: 80 callbacks suppressed
[ +19.522524] kauditd_printk_skb: 80 callbacks suppressed
[  +5.438732] kauditd_printk_skb: 44 callbacks suppressed
[Oct11 10:41] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=01:00:5e:00:00:01:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=17344 DF PROTO=2 
[Oct11 10:43] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=01:00:5e:00:00:01:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=27753 DF PROTO=2 
[Oct11 10:45] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=01:00:5e:00:00:01:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=35070 DF PROTO=2 
[Oct11 10:47] [UFW BLOCK] IN=wlp0s20f3 OUT= MAC=a4:42:3b:82:97:13:08:aa:89:48:2f:b4:08:00 SRC=192.168.1.1 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0xE0 TTL=1 ID=35566 DF PROTO=2 

* 
* ==> etcd [1dd1073146d3] <==
* {"level":"info","ts":"2022-10-11T10:49:29.139Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-10-11T10:49:29.139Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-10-11T10:49:29.139Z","caller":"embed/etcd.go:479","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-10-11T10:49:29.139Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-10-11T10:49:29.140Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.4","git-sha":"08407ff76","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-10-11T10:49:29.143Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.031516ms"}
{"level":"info","ts":"2022-10-11T10:49:29.190Z","caller":"etcdserver/raft.go:448","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2022-10-11T10:49:29.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2022-10-11T10:49:29.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2022-10-11T10:49:29.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2022-10-11T10:49:29.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2022-10-11T10:49:29.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2022-10-11T10:49:29.195Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-10-11T10:49:29.198Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2022-10-11T10:49:29.200Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-10-11T10:49:29.202Z","caller":"etcdserver/server.go:851","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.4","cluster-version":"to_be_decided"}
{"level":"info","ts":"2022-10-11T10:49:29.203Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-10-11T10:49:29.205Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2022-10-11T10:49:29.205Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-10-11T10:49:29.211Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-10-11T10:49:29.212Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-10-11T10:49:29.212Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-10-11T10:49:29.212Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-10-11T10:49:29.212Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-10-11T10:49:29.891Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2022-10-11T10:49:29.892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2022-10-11T10:49:29.892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2022-10-11T10:49:29.892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2022-10-11T10:49:29.892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-10-11T10:49:29.892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2022-10-11T10:49:29.892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-10-11T10:49:29.892Z","caller":"etcdserver/server.go:2507","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-11T10:49:29.897Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-11T10:49:29.897Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-11T10:49:29.897Z","caller":"etcdserver/server.go:2531","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-11T10:49:29.897Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-10-11T10:49:29.897Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-10-11T10:49:29.897Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-10-11T10:49:29.898Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-10-11T10:49:29.899Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-10-11T10:49:29.945Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-10-11T10:49:29.947Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}

* 
* ==> kernel <==
*  10:58:08 up 10:26,  0 users,  load average: 2.55, 3.31, 3.29
Linux minikube 5.15.0-50-generic #56~20.04.1-Ubuntu SMP Tue Sep 27 15:51:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [136e7ee1f091] <==
* I1011 10:49:32.631214       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1011 10:49:32.632012       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1011 10:49:32.840438       1 genericapiserver.go:656] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1011 10:49:34.100289       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1011 10:49:34.100374       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1011 10:49:34.100524       1 secure_serving.go:210] Serving securely on [::]:8443
I1011 10:49:34.100582       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1011 10:49:34.100849       1 autoregister_controller.go:141] Starting autoregister controller
I1011 10:49:34.100862       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1011 10:49:34.100884       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1011 10:49:34.101004       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1011 10:49:34.101230       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1011 10:49:34.102975       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1011 10:49:34.102991       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1011 10:49:34.103282       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1011 10:49:34.103292       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I1011 10:49:34.103879       1 available_controller.go:491] Starting AvailableConditionController
I1011 10:49:34.103890       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1011 10:49:34.103908       1 apf_controller.go:300] Starting API Priority and Fairness config controller
I1011 10:49:34.100295       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1011 10:49:34.119807       1 controller.go:83] Starting OpenAPI AggregationController
I1011 10:49:34.120097       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1011 10:49:34.120106       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I1011 10:49:34.120154       1 controller.go:85] Starting OpenAPI controller
I1011 10:49:34.120183       1 controller.go:85] Starting OpenAPI V3 controller
I1011 10:49:34.120201       1 naming_controller.go:291] Starting NamingConditionController
I1011 10:49:34.120224       1 establishing_controller.go:76] Starting EstablishingController
I1011 10:49:34.120240       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1011 10:49:34.120255       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1011 10:49:34.120281       1 crd_finalizer.go:266] Starting CRDFinalizer
I1011 10:49:34.134504       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1011 10:49:34.134921       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1011 10:49:34.145171       1 controller.go:616] quota admission added evaluator for: namespaces
I1011 10:49:34.201191       1 cache.go:39] Caches are synced for autoregister controller
I1011 10:49:34.203898       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I1011 10:49:34.208310       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1011 10:49:34.210635       1 apf_controller.go:305] Running API Priority and Fairness config worker
I1011 10:49:34.210745       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1011 10:49:34.223086       1 shared_informer.go:262] Caches are synced for crd-autoregister
I1011 10:49:34.232362       1 shared_informer.go:262] Caches are synced for node_authorizer
I1011 10:49:34.908978       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1011 10:49:35.132659       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1011 10:49:35.150697       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1011 10:49:35.151650       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1011 10:49:35.507964       1 controller.go:616] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1011 10:49:35.531644       1 controller.go:616] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1011 10:49:35.680597       1 alloc.go:327] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1011 10:49:35.701008       1 lease.go:250] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1011 10:49:35.704583       1 controller.go:616] quota admission added evaluator for: endpoints
I1011 10:49:35.727268       1 controller.go:616] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1011 10:49:36.193939       1 controller.go:616] quota admission added evaluator for: serviceaccounts
I1011 10:49:37.378553       1 controller.go:616] quota admission added evaluator for: deployments.apps
I1011 10:49:37.384892       1 alloc.go:327] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1011 10:49:37.392414       1 controller.go:616] quota admission added evaluator for: daemonsets.apps
I1011 10:49:37.619370       1 controller.go:616] quota admission added evaluator for: leases.coordination.k8s.io
I1011 10:49:49.491281       1 controller.go:616] quota admission added evaluator for: replicasets.apps
I1011 10:49:49.691373       1 controller.go:616] quota admission added evaluator for: controllerrevisions.apps
I1011 10:49:49.691371       1 controller.go:616] quota admission added evaluator for: controllerrevisions.apps
I1011 10:51:36.087756       1 alloc.go:327] "allocated clusterIPs" service="default/nginx-service" clusterIPs=map[IPv4:10.105.51.16]
I1011 10:51:39.022131       1 alloc.go:327] "allocated clusterIPs" service="default/django-k8s-web-service" clusterIPs=map[IPv4:10.103.249.75]

* 
* ==> kube-controller-manager [cae214108a05] <==
* I1011 10:49:49.057807       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I1011 10:49:49.057813       1 shared_informer.go:262] Caches are synced for ReplicaSet
I1011 10:49:49.064359       1 shared_informer.go:262] Caches are synced for persistent volume
I1011 10:49:49.071479       1 shared_informer.go:262] Caches are synced for ReplicationController
I1011 10:49:49.073863       1 shared_informer.go:262] Caches are synced for TTL after finished
I1011 10:49:49.134477       1 shared_informer.go:262] Caches are synced for disruption
I1011 10:49:49.144484       1 shared_informer.go:262] Caches are synced for resource quota
I1011 10:49:49.149662       1 shared_informer.go:262] Caches are synced for taint
I1011 10:49:49.149865       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
W1011 10:49:49.149984       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1011 10:49:49.150072       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1011 10:49:49.150510       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
I1011 10:49:49.150581       1 taint_manager.go:209] "Sending events to api server"
I1011 10:49:49.151219       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1011 10:49:49.160235       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1011 10:49:49.160257       1 event.go:294] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1011 10:49:49.162071       1 shared_informer.go:262] Caches are synced for endpoint
I1011 10:49:49.163055       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1011 10:49:49.170553       1 event.go:294] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1011 10:49:49.182096       1 shared_informer.go:262] Caches are synced for endpoint_slice
I1011 10:49:49.219920       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I1011 10:49:49.225120       1 shared_informer.go:262] Caches are synced for resource quota
I1011 10:49:49.492731       1 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-565d847f94 to 1"
I1011 10:49:49.573068       1 shared_informer.go:262] Caches are synced for garbage collector
I1011 10:49:49.599098       1 shared_informer.go:262] Caches are synced for garbage collector
I1011 10:49:49.599121       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1011 10:49:49.705450       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-669pp"
I1011 10:49:49.714505       1 event.go:294] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-lnnrr"
I1011 10:49:50.050307       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-565d847f94-9xrmh"
W1011 10:49:53.964169       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube-m02" does not exist
I1011 10:49:53.969997       1 range_allocator.go:367] Set node minikube-m02 PodCIDR to [10.244.1.0/24]
I1011 10:49:53.970736       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-2xtmw"
I1011 10:49:53.977247       1 event.go:294] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-zzgv4"
W1011 10:49:54.150964       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube-m02. Assuming now as a timestamp.
I1011 10:49:54.151010       1 event.go:294] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m02 event: Registered Node minikube-m02 in Controller"
I1011 10:50:09.152907       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
W1011 10:50:13.391640       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube-m03" does not exist
I1011 10:50:13.398347       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-7srp2"
I1011 10:50:13.406651       1 event.go:294] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-7vv9w"
I1011 10:50:13.413587       1 range_allocator.go:367] Set node minikube-m03 PodCIDR to [10.244.2.0/24]
W1011 10:50:14.153302       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube-m03. Assuming now as a timestamp.
I1011 10:50:14.153417       1 event.go:294] "Event occurred" object="minikube-m03" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m03 event: Registered Node minikube-m03 in Controller"
W1011 10:50:14.593426       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
W1011 10:50:33.692130       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
W1011 10:50:35.604938       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube-m04" does not exist
W1011 10:50:35.607407       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
I1011 10:50:35.647072       1 event.go:294] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-xstdv"
I1011 10:50:35.647472       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-d8mrk"
I1011 10:50:35.682795       1 range_allocator.go:367] Set node minikube-m04 PodCIDR to [10.244.3.0/24]
I1011 10:50:39.172739       1 event.go:294] "Event occurred" object="minikube-m04" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m04 event: Registered Node minikube-m04 in Controller"
W1011 10:50:39.172805       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube-m04. Assuming now as a timestamp.
W1011 10:50:56.029334       1 topologycache.go:199] Can't get CPU or zone information for minikube-m02 node
I1011 10:51:35.977796       1 event.go:294] "Event occurred" object="default/nginx-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set nginx-deployment-596f96d776 to 3"
I1011 10:51:36.018322       1 event.go:294] "Event occurred" object="default/nginx-deployment-596f96d776" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-deployment-596f96d776-bbb25"
I1011 10:51:36.032966       1 event.go:294] "Event occurred" object="default/nginx-deployment-596f96d776" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-deployment-596f96d776-6kbm4"
I1011 10:51:36.094741       1 event.go:294] "Event occurred" object="default/nginx-deployment-596f96d776" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-deployment-596f96d776-j5p29"
I1011 10:51:38.947568       1 event.go:294] "Event occurred" object="default/django-k8s-web-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set django-k8s-web-deployment-fb59f8d56 to 3"
I1011 10:51:38.961991       1 event.go:294] "Event occurred" object="default/django-k8s-web-deployment-fb59f8d56" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: django-k8s-web-deployment-fb59f8d56-699dk"
I1011 10:51:38.969366       1 event.go:294] "Event occurred" object="default/django-k8s-web-deployment-fb59f8d56" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: django-k8s-web-deployment-fb59f8d56-q8pc9"
I1011 10:51:38.969559       1 event.go:294] "Event occurred" object="default/django-k8s-web-deployment-fb59f8d56" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: django-k8s-web-deployment-fb59f8d56-m5szf"

* 
* ==> kube-proxy [390ec4a19364] <==
* I1011 10:49:50.594460       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1011 10:49:50.594525       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1011 10:49:50.594560       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1011 10:49:50.620012       1 server_others.go:206] "Using iptables Proxier"
I1011 10:49:50.620060       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1011 10:49:50.620073       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1011 10:49:50.620087       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1011 10:49:50.620102       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1011 10:49:50.620260       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1011 10:49:50.621187       1 server.go:661] "Version info" version="v1.25.0"
I1011 10:49:50.621201       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1011 10:49:50.622397       1 config.go:317] "Starting service config controller"
I1011 10:49:50.622415       1 shared_informer.go:255] Waiting for caches to sync for service config
I1011 10:49:50.622437       1 config.go:226] "Starting endpoint slice config controller"
I1011 10:49:50.622441       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1011 10:49:50.624275       1 config.go:444] "Starting node config controller"
I1011 10:49:50.624299       1 shared_informer.go:255] Waiting for caches to sync for node config
I1011 10:49:50.723082       1 shared_informer.go:262] Caches are synced for service config
I1011 10:49:50.723090       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1011 10:49:50.725156       1 shared_informer.go:262] Caches are synced for node config

* 
* ==> kube-scheduler [18065028f196] <==
* W1011 10:49:34.138095       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1011 10:49:34.151742       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.0"
I1011 10:49:34.151766       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1011 10:49:34.152839       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1011 10:49:34.152867       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1011 10:49:34.152975       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1011 10:49:34.153029       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1011 10:49:34.155837       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1011 10:49:34.155961       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1011 10:49:34.156015       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1011 10:49:34.156029       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1011 10:49:34.156081       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1011 10:49:34.156095       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1011 10:49:34.156159       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1011 10:49:34.156244       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1011 10:49:34.156252       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1011 10:49:34.156259       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1011 10:49:34.156291       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1011 10:49:34.156301       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1011 10:49:34.156187       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1011 10:49:34.156381       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1011 10:49:34.156395       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1011 10:49:34.156428       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1011 10:49:34.156440       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1011 10:49:34.156428       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1011 10:49:34.156496       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1011 10:49:34.156506       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1011 10:49:34.156555       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1011 10:49:34.156566       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1011 10:49:34.156620       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1011 10:49:34.156631       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1011 10:49:34.156927       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1011 10:49:34.156943       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1011 10:49:34.156965       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1011 10:49:34.157026       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1011 10:49:34.157192       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1011 10:49:34.157219       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1011 10:49:35.051201       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1011 10:49:35.051304       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1011 10:49:35.051554       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1011 10:49:35.051638       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1011 10:49:35.145411       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1011 10:49:35.145729       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1011 10:49:35.207560       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1011 10:49:35.207593       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1011 10:49:35.216838       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1011 10:49:35.217003       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1011 10:49:35.224827       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1011 10:49:35.224862       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1011 10:49:35.237112       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1011 10:49:35.237278       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1011 10:49:35.279456       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1011 10:49:35.279480       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1011 10:49:35.325312       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1011 10:49:35.325352       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1011 10:49:35.340059       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1011 10:49:35.340079       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1011 10:49:35.366984       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1011 10:49:35.367010       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
I1011 10:49:38.088690       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Tue 2022-10-11 10:49:14 UTC, end at Tue 2022-10-11 10:58:09 UTC. --
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.037600    1965 kubelet.go:2010] "Starting kubelet main sync loop"
Oct 11 10:49:38 minikube kubelet[1965]: E1011 10:49:38.037958    1965 kubelet.go:2034] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Oct 11 10:49:38 minikube kubelet[1965]: E1011 10:49:38.138509    1965 kubelet.go:2034] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
Oct 11 10:49:38 minikube kubelet[1965]: E1011 10:49:38.339706    1965 kubelet.go:2034] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.468311    1965 apiserver.go:52] "Watching apiserver"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.552894    1965 cpu_manager.go:213] "Starting CPU manager" policy="none"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.553833    1965 cpu_manager.go:214] "Reconciling" reconcilePeriod="10s"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.553918    1965 state_mem.go:36] "Initialized new in-memory state store"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.554543    1965 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.554595    1965 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.554624    1965 policy_none.go:49] "None policy: Start"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.593412    1965 memory_manager.go:168] "Starting memorymanager" policy="None"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.593477    1965 state_mem.go:35] "Initializing new in-memory state store"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.593778    1965 state_mem.go:75] "Updated machine memory state"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.597171    1965 manager.go:447] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.597523    1965 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.740969    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.741201    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.741321    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.741446    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.810730    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/16de73f898f4460d96d28cf19ba8407f-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"16de73f898f4460d96d28cf19ba8407f\") " pod="kube-system/kube-apiserver-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.810822    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/77bee6e3b99b4016b81d7d949c58a789-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"77bee6e3b99b4016b81d7d949c58a789\") " pod="kube-system/kube-controller-manager-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.810883    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/77bee6e3b99b4016b81d7d949c58a789-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"77bee6e3b99b4016b81d7d949c58a789\") " pod="kube-system/kube-controller-manager-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.810936    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/77bee6e3b99b4016b81d7d949c58a789-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"77bee6e3b99b4016b81d7d949c58a789\") " pod="kube-system/kube-controller-manager-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.810995    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/77bee6e3b99b4016b81d7d949c58a789-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"77bee6e3b99b4016b81d7d949c58a789\") " pod="kube-system/kube-controller-manager-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811051    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/77bee6e3b99b4016b81d7d949c58a789-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"77bee6e3b99b4016b81d7d949c58a789\") " pod="kube-system/kube-controller-manager-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811105    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/b564e2b429a630c74cbb01d3c4b7a228-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"b564e2b429a630c74cbb01d3c4b7a228\") " pod="kube-system/kube-scheduler-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811161    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/16de73f898f4460d96d28cf19ba8407f-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"16de73f898f4460d96d28cf19ba8407f\") " pod="kube-system/kube-apiserver-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811215    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/16de73f898f4460d96d28cf19ba8407f-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"16de73f898f4460d96d28cf19ba8407f\") " pod="kube-system/kube-apiserver-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811266    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/16de73f898f4460d96d28cf19ba8407f-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"16de73f898f4460d96d28cf19ba8407f\") " pod="kube-system/kube-apiserver-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811312    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/16de73f898f4460d96d28cf19ba8407f-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"16de73f898f4460d96d28cf19ba8407f\") " pod="kube-system/kube-apiserver-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811375    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/77bee6e3b99b4016b81d7d949c58a789-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"77bee6e3b99b4016b81d7d949c58a789\") " pod="kube-system/kube-controller-manager-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811424    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/77bee6e3b99b4016b81d7d949c58a789-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"77bee6e3b99b4016b81d7d949c58a789\") " pod="kube-system/kube-controller-manager-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811472    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/bd495b7643dfc9d3194bd002e968bc3d-etcd-certs\") pod \"etcd-minikube\" (UID: \"bd495b7643dfc9d3194bd002e968bc3d\") " pod="kube-system/etcd-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811518    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/bd495b7643dfc9d3194bd002e968bc3d-etcd-data\") pod \"etcd-minikube\" (UID: \"bd495b7643dfc9d3194bd002e968bc3d\") " pod="kube-system/etcd-minikube"
Oct 11 10:49:38 minikube kubelet[1965]: I1011 10:49:38.811536    1965 reconciler.go:169] "Reconciler: start to sync state"
Oct 11 10:49:38 minikube kubelet[1965]: E1011 10:49:38.812381    1965 kubelet.go:1712] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Oct 11 10:49:43 minikube kubelet[1965]: E1011 10:49:43.610777    1965 kubelet.go:2373] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"
Oct 11 10:49:48 minikube kubelet[1965]: E1011 10:49:48.645204    1965 kubelet.go:2373] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.076748    1965 kuberuntime_manager.go:1050] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.077286    1965 kubelet_network.go:60] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 11 10:49:49 minikube kubelet[1965]: E1011 10:49:49.088965    1965 kubelet.go:2373] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.712112    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.724084    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809156    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/92f1debf-5037-4858-8858-72a7f78ee74f-lib-modules\") pod \"kube-proxy-669pp\" (UID: \"92f1debf-5037-4858-8858-72a7f78ee74f\") " pod="kube-system/kube-proxy-669pp"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809205    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xd78s\" (UniqueName: \"kubernetes.io/projected/92f1debf-5037-4858-8858-72a7f78ee74f-kube-api-access-xd78s\") pod \"kube-proxy-669pp\" (UID: \"92f1debf-5037-4858-8858-72a7f78ee74f\") " pod="kube-system/kube-proxy-669pp"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809232    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/92f1debf-5037-4858-8858-72a7f78ee74f-xtables-lock\") pod \"kube-proxy-669pp\" (UID: \"92f1debf-5037-4858-8858-72a7f78ee74f\") " pod="kube-system/kube-proxy-669pp"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809255    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/23b562ce-a6b5-46cd-b4e7-cc3effc97944-cni-cfg\") pod \"kindnet-lnnrr\" (UID: \"23b562ce-a6b5-46cd-b4e7-cc3effc97944\") " pod="kube-system/kindnet-lnnrr"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809313    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vwhrx\" (UniqueName: \"kubernetes.io/projected/23b562ce-a6b5-46cd-b4e7-cc3effc97944-kube-api-access-vwhrx\") pod \"kindnet-lnnrr\" (UID: \"23b562ce-a6b5-46cd-b4e7-cc3effc97944\") " pod="kube-system/kindnet-lnnrr"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809352    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/92f1debf-5037-4858-8858-72a7f78ee74f-kube-proxy\") pod \"kube-proxy-669pp\" (UID: \"92f1debf-5037-4858-8858-72a7f78ee74f\") " pod="kube-system/kube-proxy-669pp"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809402    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/23b562ce-a6b5-46cd-b4e7-cc3effc97944-xtables-lock\") pod \"kindnet-lnnrr\" (UID: \"23b562ce-a6b5-46cd-b4e7-cc3effc97944\") " pod="kube-system/kindnet-lnnrr"
Oct 11 10:49:49 minikube kubelet[1965]: I1011 10:49:49.809429    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/23b562ce-a6b5-46cd-b4e7-cc3effc97944-lib-modules\") pod \"kindnet-lnnrr\" (UID: \"23b562ce-a6b5-46cd-b4e7-cc3effc97944\") " pod="kube-system/kindnet-lnnrr"
Oct 11 10:49:50 minikube kubelet[1965]: I1011 10:49:50.614970    1965 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="0fe3bb2bd5358bd4f20e9cbc2dbf73d206174b0e795395c22e75fcf99d431840"
Oct 11 10:49:53 minikube kubelet[1965]: E1011 10:49:53.651829    1965 kubelet.go:2373] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"
Oct 11 10:50:08 minikube kubelet[1965]: I1011 10:50:08.493607    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:50:08 minikube kubelet[1965]: I1011 10:50:08.498283    1965 topology_manager.go:205] "Topology Admit Handler"
Oct 11 10:50:08 minikube kubelet[1965]: I1011 10:50:08.546316    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/4d69eaed-dd5f-4d90-92f5-96c9099c8424-config-volume\") pod \"coredns-565d847f94-9xrmh\" (UID: \"4d69eaed-dd5f-4d90-92f5-96c9099c8424\") " pod="kube-system/coredns-565d847f94-9xrmh"
Oct 11 10:50:08 minikube kubelet[1965]: I1011 10:50:08.546359    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/89d60b28-378f-4e8a-925e-2d1967a691db-tmp\") pod \"storage-provisioner\" (UID: \"89d60b28-378f-4e8a-925e-2d1967a691db\") " pod="kube-system/storage-provisioner"
Oct 11 10:50:08 minikube kubelet[1965]: I1011 10:50:08.546393    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kvxvz\" (UniqueName: \"kubernetes.io/projected/4d69eaed-dd5f-4d90-92f5-96c9099c8424-kube-api-access-kvxvz\") pod \"coredns-565d847f94-9xrmh\" (UID: \"4d69eaed-dd5f-4d90-92f5-96c9099c8424\") " pod="kube-system/coredns-565d847f94-9xrmh"
Oct 11 10:50:08 minikube kubelet[1965]: I1011 10:50:08.546421    1965 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cvcl9\" (UniqueName: \"kubernetes.io/projected/89d60b28-378f-4e8a-925e-2d1967a691db-kube-api-access-cvcl9\") pod \"storage-provisioner\" (UID: \"89d60b28-378f-4e8a-925e-2d1967a691db\") " pod="kube-system/storage-provisioner"

* 
* ==> storage-provisioner [e842a88d6eca] <==
* I1011 10:50:09.513398       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1011 10:50:09.524857       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1011 10:50:09.525094       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1011 10:50:09.534653       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1011 10:50:09.534693       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"8635f110-52b1-495b-b557-4c55d00d6e5a", APIVersion:"v1", ResourceVersion:"453", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e9eddda3-1236-496b-8ef2-cce23a2b1d71 became leader
I1011 10:50:09.534973       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e9eddda3-1236-496b-8ef2-cce23a2b1d71!
I1011 10:50:09.635785       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e9eddda3-1236-496b-8ef2-cce23a2b1d71!

